{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet-18\n",
    "**How to write a ResNet-18 with pytorch**\n",
    "\n",
    "<br>\n",
    "The following cells contain codes for building a ResNet-18 in pytorch, and training it on the CIFAR-10 dataset. Then, training losses and hidden layers at the end of training are visualised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn import Conv2d, MaxPool2d\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define resnet building blocks\n",
    "\n",
    "class ResidualBlock(nn.Module): \n",
    "    def __init__(self, inchannel, outchannel, stride=1): \n",
    "        \n",
    "        super(ResidualBlock, self).__init__() \n",
    "        \n",
    "        self.left = nn.Sequential(Conv2d(inchannel, outchannel, kernel_size=3, \n",
    "                                         stride=stride, padding=1, bias=False), \n",
    "                                  nn.BatchNorm2d(outchannel), \n",
    "                                  nn.ReLU(inplace=True), \n",
    "                                  Conv2d(outchannel, outchannel, kernel_size=3, \n",
    "                                         stride=1, padding=1, bias=False), \n",
    "                                  nn.BatchNorm2d(outchannel)) \n",
    "        \n",
    "        self.shortcut = nn.Sequential() \n",
    "        \n",
    "        if stride != 1 or inchannel != outchannel: \n",
    "            \n",
    "            self.shortcut = nn.Sequential(Conv2d(inchannel, outchannel, \n",
    "                                                 kernel_size=1, stride=stride, \n",
    "                                                 padding = 0, bias=False), \n",
    "                                          nn.BatchNorm2d(outchannel) ) \n",
    "            \n",
    "    def forward(self, x): \n",
    "        \n",
    "        out = self.left(x) \n",
    "        \n",
    "        out += self.shortcut(x) \n",
    "        \n",
    "        out = F.relu(out) \n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "    \n",
    "    # define resnet\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, ResidualBlock, num_classes = 10):\n",
    "        \n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(Conv2d(3, 64, kernel_size = 3, stride = 1,\n",
    "                                            padding = 1, bias = False), \n",
    "                                  nn.BatchNorm2d(64), \n",
    "                                  nn.ReLU())\n",
    "        \n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64, 2, stride = 1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride = 2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride = 2)\n",
    "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride = 2)\n",
    "        self.maxpool = MaxPool2d(4)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    \n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        \n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        for stride in strides:\n",
    "            \n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            \n",
    "            self.inchannel = channels\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "def ResNet18():\n",
    "    return ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensor and normalise\n",
    "# then do image augmentation by flipping samples at random with probability 0.2\n",
    "train_transform = T.Compose(\n",
    "        [T.ToTensor(), T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        T.RandomHorizontalFlip(p=0.2)])\n",
    "\n",
    "test_transform = T.Compose(\n",
    "        [T.ToTensor(), T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_dir = './data'\n",
    "batch_size = 8\n",
    "# transform and load the dataset\n",
    "train_dataset = dset.CIFAR10(root=data_dir, train=True,\n",
    "                                            download = True, transform = train_transform)\n",
    "test_dataset = dset.CIFAR10(root=data_dir, train=False,\n",
    "                                            download = True, transform = test_transform)\n",
    "# create data loaders\n",
    "loader_train = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle = True)\n",
    "loader_test = DataLoader(test_dataset, batch_size = batch_size,\n",
    "                                         shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 \n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "print_every = 100\n",
    "\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    # function for test accuracy on validation and test set\n",
    "    \n",
    "#     if loader.dataset.train:\n",
    "#         print('Checking accuracy on validation set')\n",
    "#     else:\n",
    "#         print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        # declare accuracy to be global\n",
    "        global acc\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "        \n",
    "\n",
    "def train_part(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        print(len(loader_train))\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters of the model using the gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Epoch: %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
    "                # store the loss for plotting\n",
    "                losses.append(loss.item())\n",
    "                epochh.append(e+1)\n",
    "                #check_accuracy(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimising network performance\n",
    "\n",
    "# hyperparameter tuning with random search\n",
    "# the same process is done separately for both Adam and SGD\n",
    "# when SGD is the optimiser, momentum is tuned as well\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "\n",
    "# store the hyperparameters in lists\n",
    "lr = []\n",
    "batchsize = []\n",
    "weightdecay = []\n",
    "accuracy = []\n",
    "\n",
    "# train 25 models by randomly sampling hyperparameters\n",
    "# after finding the best model, zoom into the neighbourhood of its hyperparameter space\n",
    "# and perform subsequent rounds of random search\n",
    "# lastly, hyperparameters like epoch are also tuned individually to achieve better validation performance\n",
    "num_model = 25\n",
    "for i in range(num_model):\n",
    "    learning_rate = 10**(-2*np.random.rand()-2)  #10^-4 to 10^-2, sampling from log scale\n",
    "    lr.append(learning_rate)\n",
    "\n",
    "    batch = np.array([2,4,8,16,32,64])\n",
    "    batch_size = int(np.random.choice(batch))\n",
    "    batchsize.append(batch_size)\n",
    "\n",
    "    weight_decay = 10**(-2*np.random.rand()-2)  # 10^-4 to 10^-2, sampling from log scale\n",
    "    weightdecay.append(weight_decay)\n",
    "\n",
    "    # split data into training and validation set\n",
    "    torch.manual_seed(5)\n",
    "    val_size = 10000\n",
    "    train_size = 40000\n",
    "    # I did not do cross validation here because both the training and validation sets\n",
    "    # have a large number of observations, and training data already underwent augmentation.\n",
    "    # The validation accuracy should be a good enough indicator of the model's performance.\n",
    "    train, val = random_split(train_dataset, [train_size, val_size])\n",
    "  \n",
    "    # load data into loader\n",
    "    loader_train = DataLoader(train, batch_size=batch_size,\n",
    "                                             shuffle = True)\n",
    "    loader_val = DataLoader(val, batch_size=batch_size,\n",
    "                                             shuffle = False)\n",
    "\n",
    "    # define and train the network\n",
    "    model = ResNet18()\n",
    "    # define optimiser and relevant hyperparameters here\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "    # training loop\n",
    "    train_part(model, optimizer, epochs = 10)\n",
    "    acc = 0\n",
    "    check_accuracy(loader_val, model)\n",
    "    accuracy.append(acc)\n",
    "    print(acc)\n",
    "\n",
    "\n",
    "# view results in pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "print(lr)\n",
    "print(batchsize)\n",
    "print(weightdecay)\n",
    "print(accuracy)\n",
    "\n",
    "result = {'lr': lr, 'batch_size': batchsize, 'weight_decay': weightdecay, 'accuracy': accuracy}\n",
    "df = pd.DataFrame(result)\n",
    "df = df.sort_values(by = ['accuracy'], ascending = False)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and train the network with the optimal hyperparameters \n",
    "# lr = 0.0001, batch_size = 8, flipping probability = 0.2\n",
    "# weight_decay = 0.0008, optimiser = Adam, epoch = 10\n",
    "\n",
    "model = ResNet18()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay=0.0008)\n",
    "\n",
    "losses = []\n",
    "epochh = []\n",
    "train_part(model, optimizer, epochs = 10)\n",
    "\n",
    "\n",
    "# report test set accuracy\n",
    "\n",
    "check_accuracy(loader_test, model) # test accuracy at 86.15%\n",
    "\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), 'model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "!pip install matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(epochh, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the hidden layers\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "vis_labels = ['conv1', 'layer1', 'layer2', 'layer3', 'layer4']\n",
    "\n",
    "for l in vis_labels:\n",
    "\n",
    "    getattr(model, l).register_forward_hook(get_activation(l))\n",
    "    \n",
    "    \n",
    "# data, _ = cifar10_test[0]\n",
    "data, _ = test_dataset[0]\n",
    "data = data.unsqueeze_(0).to(device = device, dtype = dtype)\n",
    "\n",
    "output = model(data)\n",
    "\n",
    "for idx, l in enumerate(vis_labels):\n",
    "\n",
    "    act = activation[l].squeeze()\n",
    "\n",
    "    if idx < 2:\n",
    "        ncols = 8\n",
    "    else:\n",
    "        ncols = 32\n",
    "        \n",
    "    nrows = act.size(0) // ncols\n",
    "    \n",
    "    fig, axarr = plt.subplots(nrows, ncols)\n",
    "    fig.suptitle(l)\n",
    "\n",
    "\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            axarr[i, j].imshow(act[i * nrows + j].cpu())\n",
    "            axarr[i, j].axis('off')\n",
    "\n",
    "    plt.savefig(f'{l}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
