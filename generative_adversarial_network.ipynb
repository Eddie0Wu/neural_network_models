{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative adversarial network (GAN)\n",
    "\n",
    "The following cells contain the codes for building a generative adversarial network in Pytorch, and training it on CIFAR10. It is recommended to run this in Google Colab as training will take a really long time without GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up\n",
    "!pip install -q torch torchvision altair matplotlib pandas\n",
    "!git clone -q https://github.com/afspies/icl_dl_cw2_utils\n",
    "from icl_dl_cw2_utils.utils.plotting import plot_tsne\n",
    "%load_ext google.colab.data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive') # Outputs will be saved in your google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def denorm(x, channels=None, w=None ,h=None, resize = False):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    if resize:\n",
    "        if channels is None or w is None or h is None:\n",
    "            print('Number of channels, width and height must be provided for resize.')\n",
    "        x = x.view(x.size(0), channels, w, h)\n",
    "    return x\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.cpu().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "\n",
    "if not os.path.exists('/content/drive/MyDrive/icl_dl_cw2/CW_GAN'):\n",
    "    os.makedirs('/content/drive/MyDrive/icl_dl_cw2/CW_GAN')\n",
    "\n",
    "GPU = True # Choose whether to use GPU\n",
    "if GPU:\n",
    "    device = torch.device(\"cuda\"  if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f'Using {device}')\n",
    "\n",
    "# Set a random seed to ensure that the results are reproducible.\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "batch_size = 128  # change that\n",
    "\n",
    "transform = transforms.Compose([\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),),                        \n",
    "])\n",
    "\n",
    "data_dir = './datasets'\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(data_dir, train=True, download=True, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_dir, train=False, download=True, transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=batch_size)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise data\n",
    "samples, _ = next(iter(loader_test))\n",
    "\n",
    "samples = samples.cpu()\n",
    "samples = make_grid(denorm(samples), nrow=8, padding=2, normalize=False,\n",
    "                        range=None, scale_each=False, pad_value=0)\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.axis('off')\n",
    "show(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the number of epochs, the learning rate\n",
    "# and the size of the Generator's input noise vetor.\n",
    "\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0001\n",
    "latent_vector_size = 100\n",
    "\n",
    "#Â Other hyperparams\n",
    "num_feature_maps = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GAN\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.gen = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_vector_size, num_feature_maps*4, 4, 1, 0, bias = False),\n",
    "            nn.BatchNorm2d(num_feature_maps*4),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(num_feature_maps*4, num_feature_maps*2, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(num_feature_maps*2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(num_feature_maps*2, num_feature_maps, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(num_feature_maps),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(num_feature_maps, 3, 4, 2, 1, bias = False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, z, label = None):\n",
    "        \n",
    "        out = self.gen(z)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.dis = nn.Sequential(\n",
    "            nn.Conv2d(3, num_feature_maps, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(num_feature_maps),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(num_feature_maps, num_feature_maps*2, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(num_feature_maps*2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(num_feature_maps*2, num_feature_maps*4, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(num_feature_maps*4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(num_feature_maps*4, 1, 4, 1, 0, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x, label = None):\n",
    "     \n",
    "        out = self.dis(x)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_weights_init = True\n",
    "\n",
    "model_G = Generator().to(device)\n",
    "if use_weights_init:\n",
    "    model_G.apply(weights_init)\n",
    "params_G = sum(p.numel() for p in model_G.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters in Generator is: {}\".format(params_G))\n",
    "print(model_G)\n",
    "print('\\n')\n",
    "\n",
    "model_D = Discriminator().to(device)\n",
    "if use_weights_init:\n",
    "    model_D.apply(weights_init)\n",
    "params_D = sum(p.numel() for p in model_D.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters in Discriminator is: {}\".format(params_D))\n",
    "print(model_D)\n",
    "print('\\n')\n",
    "\n",
    "print(\"Total number of parameters is: {}\".format(params_G + params_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(out, label):\n",
    "    loss = F.binary_cross_entropy(out, label)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "beta1 = 0.5\n",
    "optimizerD = torch.optim.Adam(model_D.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(model_G.parameters(), lr=learning_rate, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the losses\n",
    "train_losses_D = []\n",
    "train_losses_D_real = []\n",
    "train_losses_D_fake = []\n",
    "train_losses_G = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss_D = 0\n",
    "    train_loss_D_real = 0\n",
    "    train_loss_D_fake = 0\n",
    "    train_loss_G = 0\n",
    "\n",
    "    for i, data in enumerate(loader_train, 0):\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################device\n",
    "        \n",
    "        # train with real\n",
    "        model_D.zero_grad()\n",
    "\n",
    "        real_cpu = data[0].to(device = device)\n",
    "            # create the labels\n",
    "        batch = real_cpu.shape[0]\n",
    "        label = torch.full((batch,), real_label, dtype = torch.float, device = device)\n",
    "        out = model_D(real_cpu).view(-1)\n",
    "            # calculate loss and propagate back\n",
    "        real_error = loss_function(out, label)\n",
    "        real_error.backward()\n",
    "\n",
    "        D_x = out.mean().item()\n",
    "\n",
    "\n",
    "        # train with fake\n",
    "        fixed_noise = torch.randn(batch, latent_vector_size, 1, 1, device=device)\n",
    "            # generate fake samples\n",
    "        fake = model_G(fixed_noise)\n",
    "        label.fill_(fake_label)\n",
    "        out = model_D(fake.detach()).view(-1)\n",
    "\n",
    "        fake_error = loss_function(out, label)\n",
    "        fake_error.backward()\n",
    "\n",
    "        D_G_z1 = out.mean().item()\n",
    "\n",
    "        errD = real_error + fake_error  \n",
    "\n",
    "        optimizerD.step()     \n",
    "            # keep track of the losses for plotting\n",
    "        train_loss_D += errD.item()\n",
    "        train_loss_D_real += real_error.item()\n",
    "        train_loss_D_fake += fake_error.item()\n",
    "\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        model_G.zero_grad()\n",
    "            # generate labels to be real\n",
    "        label.fill_(real_label)\n",
    "        out = model_D(fake).view(-1)\n",
    "\n",
    "            # calculate loss and propagate back\n",
    "        errG = loss_function(out, label)\n",
    "        errG.backward()\n",
    "\n",
    "        D_G_z2 = out.mean().item()\n",
    "            # update optimiser\n",
    "        optimizerG.step()\n",
    "            # keep track of the loss for plotting\n",
    "        train_loss_G += errG.item()\n",
    "\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, num_epochs, i, len(loader_train),\n",
    "                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "    if epoch == 0:\n",
    "        save_image(denorm(real_cpu.cpu()).float(), '/content/drive/MyDrive/GAN/real_samples.png')\n",
    "    with torch.no_grad():\n",
    "        fake = model_G(fixed_noise)\n",
    "        save_image(denorm(fake.cpu()).float(), '/content/drive/MyDrive/GAN/fake_samples_epoch_%03d.png' % epoch)\n",
    "    train_losses_D.append(train_loss_D / len(loader_train))\n",
    "    train_losses_D_real.append(train_loss_D_real / len(loader_train))\n",
    "    train_losses_D_fake.append(train_loss_D_fake / len(loader_train))\n",
    "    train_losses_G.append(train_loss_G / len(loader_train))\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "# save  models \n",
    "#Â if your discriminator/generator are conditional you'll want to change the inputs here\n",
    "torch.jit.save(torch.jit.trace(model_G, (fixed_noise)), '/content/drive/MyDrive/GAN/GAN_G_model.pth')\n",
    "torch.jit.save(torch.jit.trace(model_D, (fake)), '/content/drive/MyDrive/GAN/GAN_D_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator samples\n",
    "\n",
    "input_noise = torch.randn(100, latent_vector_size, 1, 1, device=device)\n",
    "with torch.no_grad():\n",
    "    # visualize the generated images\n",
    "    generated = model_G(input_noise).cpu()\n",
    "    generated = make_grid(denorm(generated)[:100], nrow=10, padding=2, normalize=False, \n",
    "                        range=None, scale_each=False, pad_value=0)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    save_image(generated,'/content/drive/MyDrive/GAN/final.png')\n",
    "    show(generated) # note these are now class conditional images columns rep classes 1-10\n",
    "\n",
    "it = iter(loader_test)\n",
    "sample_inputs, _ = next(it)\n",
    "fixed_input = sample_inputs[0:64, :, :, :]\n",
    "# visualize the original images of the last batch of the test set for comparison\n",
    "img = make_grid(denorm(fixed_input), nrow=8, padding=2, normalize=False,\n",
    "                range=None, scale_each=False, pad_value=0)\n",
    "plt.figure(figsize=(15,15))\n",
    "show(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loss\n",
    "\n",
    "iterations = list(range(len(train_losses_D)))\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.plot(iterations, train_losses_D, label = \"d_loss\")\n",
    "ax1.plot(iterations, train_losses_D_real, label = \"d_loss_real\")\n",
    "ax1.plot(iterations, train_losses_D_fake, label = \"d_loss_fake\")\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title(\"discriminator\")\n",
    "ax1.legend(loc='best')\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.plot(iterations, train_losses_G, label = \"g_loss\")\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_title(\"generator\")\n",
    "ax2.legend(loc='best')\n",
    "plt.savefig(\"/content/drive/MyDrive/GAN/loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
