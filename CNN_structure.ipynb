{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Structure\n",
    "**How each layer works in a vanilla CNN**\n",
    "\n",
    "<br>\n",
    "The following cells contain codes for writing Conv2d, Maxpool2d, Linear and Batchnorm2d from scratch.<br>\n",
    "They show how each type of layer in a CNN works under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2d from scratch\n",
    "class Conv2d(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride=1,\n",
    "                 padding=0,\n",
    "                 bias=True):\n",
    "\n",
    "        super(Conv2d, self).__init__()\n",
    "        \"\"\"\n",
    "        An implementation of a convolutional layer.\n",
    "\n",
    "        The input consists of N data points, each with C channels, height H and\n",
    "        width W. We convolve each input with F different filters, where each filter\n",
    "        spans all C channels and has height HH and width WW.\n",
    "\n",
    "        Parameters:\n",
    "        - w: Filter weights of shape (F, C, HH, WW)\n",
    "        - b: Biases, of shape (F,)\n",
    "        - kernel_size: Size of the convolving kernel\n",
    "        - stride: The number of pixels between adjacent receptive fields in the\n",
    "            horizontal and vertical directions.\n",
    "        - padding: The number of pixels that will be used to zero-pad the input.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.bias = bias\n",
    "        \n",
    "        # since kernel_size, stride and padding argument inputs can be either tuple or integer,\n",
    "        # set all inputs to tuple\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "        self.stride = stride if isinstance(stride, tuple) else (stride, stride)\n",
    "        self.padding = padding if isinstance(padding, tuple) else (padding, padding)\n",
    "        \n",
    "        # initialise the kernel weights with Kaiming initialisation which is \n",
    "        # popularly used in convolutional layers\n",
    "        std = (2/(out_channels*self.kernel_size[0]*self.kernel_size[1]))**0.5\n",
    "        self.weight = torch.Tensor(out_channels, in_channels, \n",
    "                                   self.kernel_size[0], self.kernel_size[1]).normal_(mean = 0, std = std)\n",
    "        # please comment out the next line if nn.Parameter() is not needed\n",
    "        self.weight = nn.Parameter(self.weight)\n",
    "        \n",
    "        # initialise bias to be zero as we do not want any bias initially in the network\n",
    "        if bias:\n",
    "            self.bias = torch.zeros((out_channels,))\n",
    "            # please comment out the next line if nn.Parameter() is not needed\n",
    "            self.bias = nn.Parameter(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        - x: Input data of shape (N, C, H, W)\n",
    "        Output:\n",
    "        - out: Output data, of shape (N, F, H', W').\n",
    "        \"\"\"\n",
    "        \n",
    "        # calculate the output image height\n",
    "        out_dimension_0 = int((x.shape[2] + 2*self.padding[0] - self.kernel_size[0])/self.stride[0] + 1)\n",
    "        # unfold the input tensor to obtain the image patches\n",
    "        x = F.unfold(x, kernel_size = self.kernel_size, padding = self.padding, stride = self.stride)\n",
    "        weight = self.weight.view(self.out_channels, -1)\n",
    "        \n",
    "        # use matrix multiplication to carry out convolution\n",
    "        x = weight @ x\n",
    "        # reshape output into the desired shape\n",
    "        out = x.view(x.shape[0], self.out_channels, out_dimension_0, -1)\n",
    "        \n",
    "        if torch.is_tensor(self.bias): # add bias if bias is True\n",
    "            out += self.bias.view(1, self.out_channels, 1, 1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maxpool2d from scratch\n",
    "class MaxPool2d(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(MaxPool2d, self).__init__()\n",
    "        \"\"\"\n",
    "        An implementation of a max-pooling layer.\n",
    "\n",
    "        Parameters:\n",
    "        - kernel_size: the size of the window to take a max over\n",
    "        \"\"\"        \n",
    "        # since kernel_size input can be either tuple or integer,\n",
    "        # set it to tuple\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        - x: Input data of shape (N, C, H, W)\n",
    "        Output:\n",
    "        - out: Output data, of shape (N, F, H', W').\n",
    "        \"\"\"\n",
    "        # store the number of channels\n",
    "        num_channels = x.shape[1]\n",
    "        # calculate the output image height\n",
    "        out_dimension_0 = int((x.shape[2]-self.kernel_size[0])/self.kernel_size[0] + 1)\n",
    "        # unfold the input tensor to obtain the image patches\n",
    "        x = F.unfold(x, kernel_size = self.kernel_size, stride = self.kernel_size)\n",
    "        x = x.view(x.shape[0], num_channels, -1, x.shape[2])\n",
    "        \n",
    "        # get the maximum\n",
    "        x = torch.max(x, dim = 2, keepdim = True).values\n",
    "        # reshape output into the desired shape\n",
    "        out = x.view(x.shape[0], num_channels, out_dimension_0, -1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear layer from scratch\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        \"\"\"\n",
    "        An implementation of a Linear layer.\n",
    "\n",
    "        Parameters:\n",
    "        - weight: the learnable weights of the module of shape (in_channels, out_channels).\n",
    "        - bias: the learnable bias of the module of shape (out_channels).\n",
    "        \"\"\"        \n",
    "        self.input = in_channels\n",
    "        self.output = out_channels\n",
    "        self.bias = bias\n",
    "        # initialise weights with Xavier Glorot initialization\n",
    "        xavier = (6/(in_channels + out_channels))**(0.5)\n",
    "        self.weight = xavier*2*torch.rand(in_channels, out_channels) - xavier\n",
    "        # please comment out the next line if nn.Parameter() is not needed\n",
    "        self.weight = nn.Parameter(self.weight)\n",
    "        \n",
    "        # initialise bias to be zero\n",
    "        if bias:\n",
    "            self.bias = torch.zeros(out_channels)\n",
    "            # please comment out the next line if nn.Parameter() is not needed\n",
    "            self.bias = nn.Parameter(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        - x: Input data of shape (N, *, H) where * means any number of additional\n",
    "        dimensions and H = in_channels\n",
    "        Output:\n",
    "        - out: Output data of shape (N, *, H') where * means any number of additional\n",
    "        dimensions and H' = out_channels\n",
    "        \"\"\"\n",
    "        \n",
    "        # matrix multiplication to get the output\n",
    "        out = x @ self.weight\n",
    "        if torch.is_tensor(self.bias): # add bias if bias is True\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batchnorm2d from scratch\n",
    "class BatchNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1):\n",
    "        super(BatchNorm2d, self).__init__()\n",
    "        \"\"\"\n",
    "        An implementation of a Batch Normalization over a mini-batch of 2D inputs.\n",
    "\n",
    "        The mean and standard-deviation are calculated per-dimension over the\n",
    "        mini-batches and gamma and beta are learnable parameter vectors of\n",
    "        size num_features.\n",
    "\n",
    "        Parameters:\n",
    "        - num_features: C from an expected input of size (N, C, H, W).\n",
    "        - eps: a value added to the denominator for numerical stability. Default: 1e-5\n",
    "        - momentum: momentum â€“ the value used for the running_mean and running_var\n",
    "        computation. Default: 0.1\n",
    "        - gamma: the learnable weights of shape (num_features).\n",
    "        - beta: the learnable bias of the module of shape (num_features).\n",
    "        \"\"\"\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        # intialise gamma and beta with standard normal distribution\n",
    "        self.gamma = torch.randn(num_features)\n",
    "        self.beta = torch.randn(num_features)\n",
    "        # please comment out the next two lines if nn.Parameter() is not needed\n",
    "        self.gamma = nn.Parameter(self.gamma)\n",
    "        self.beta = nn.Parameter(self.beta)\n",
    "        \n",
    "        # keeps track of the running mean and variance\n",
    "        self.running_mean = torch.zeros(num_features)\n",
    "        self.running_var = torch.zeros(num_features)\n",
    "        \n",
    "        # a flag to show whether the model is training or evaluating\n",
    "        self.train = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        During training this layer keeps running estimates of its computed mean and\n",
    "        variance, which are then used for normalization during evaluation.\n",
    "        Input:\n",
    "        - x: Input data of shape (N, C, H, W)\n",
    "        Output:\n",
    "        - out: Output data of shape (N, C, H, W) (same shape as input)\n",
    "        \"\"\"\n",
    "        if self.train: # during training\n",
    "            # find the mean and variance across the minibatches\n",
    "            x_mean = torch.mean(x, dim = (0,2,3), keepdim=True)\n",
    "            x_var = torch.var(x, dim = (0,2,3), keepdim=True, unbiased=False)\n",
    "\n",
    "            gamma = self.gamma.view(1, self.num_features, 1, 1)\n",
    "            beta = self.beta.view(1, self.num_features, 1, 1)\n",
    "            # normalise across the minibatches\n",
    "            out = ((x-x_mean)/((x_var+self.eps)**0.5))*gamma + beta\n",
    "            \n",
    "            # update the running mean and variance\n",
    "            x_mean = x_mean.view(self.num_features)\n",
    "            x_var = x_var.view(self.num_features)\n",
    "            self.running_mean = self.running_mean*self.momentum + x_mean*(1-self.momentum)\n",
    "            self.running_var = self.running_var*self.momentum + x_var*(1-self.momentum)\n",
    "        \n",
    "        \n",
    "        else: # during evaluation\n",
    "            gamma = self.gamma.view(1, self.num_features, 1, 1)\n",
    "            beta = self.beta.view(1, self.num_features, 1, 1)\n",
    "            # use running mean and variance for normalisation\n",
    "            x_mean = self.running_mean.view(1, self.num_features, 1, 1)\n",
    "            x_var = self.running_var.view(1, self.num_features, 1, 1)\n",
    "            out = ((x-x_mean)/((x_var+self.eps)**0.5))*gamma + beta\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
