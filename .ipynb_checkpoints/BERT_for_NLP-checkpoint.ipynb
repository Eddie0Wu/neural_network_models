{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT for sentiment analysis\n",
    "\n",
    "This notebook contains the codes for fine tuning a pre-trained BERT to predict the level of humour in texts. <br>\n",
    "It also contains codes for running models without using pre-trained representation for comparison. <br>\n",
    "Examples include CNN and and logistic regression with manual feature engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download word embedding if needed\n",
    "\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove.6B.zip\n",
    "\n",
    "! pip install torch pandas numpy sklearn wget transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training, development and test set\n",
    "# Training set also includes data from funlines\n",
    "# The links are most likely not working alrdy by now\n",
    "\n",
    "!wget -O train.csv https://www.dropbox.com/s/3hm0nf7p5yvy4dc/train.csv?dl=0\n",
    "# !wget -O train_funlines.csv https://www.dropbox.com/s/e8v4mor8sx00g0u/train_funlines.csv?dl=0\n",
    "!wget -O dev.csv https://www.dropbox.com/s/7k5j43hq8ad5ghj/dev.csv?dl=0\n",
    "!wget -O test.csv https://www.dropbox.com/s/ogqwx15uhfrinbd/test.csv?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up\n",
    "\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import codecs\n",
    "\n",
    "\n",
    "if not os.path.exists('/content/drive/MyDrive/nlp/'):\n",
    "    os.makedirs('/content/drive/MyDrive/nlp/')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting random seed and device\n",
    "SEED = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# np.random.seed(SEED)\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "dev_df = pd.read_csv(\"dev.csv\")\n",
    "print(train_df.shape)\n",
    "print(dev_df.shape)\n",
    "# train_df = pd.concat([train_df, train_funlines_df])\n",
    "# train_df.index = range(len(train_df))\n",
    "# print(train_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Dataset class\n",
    "\n",
    "class Task1Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, train_data, labels):\n",
    "        self.x_train = train_data\n",
    "        self.y_train = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.x_train[item], self.y_train[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "\n",
    "def train(train_iter, dev_iter, model, embedding_model, number_epoch, scheduler):\n",
    "    \"\"\"\n",
    "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Training model.\")\n",
    "\n",
    "    for epoch in range(number_epoch):\n",
    "        \n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_sse = 0\n",
    "        no_observations = 0    # Observations used for training so far\n",
    "        pred_train = np.empty(0)\n",
    "\n",
    "        for feature, target in train_iter:\n",
    "\n",
    "            feature, target = feature.to(device), target.float().to(device)\n",
    "            segments_tensor = torch.where(feature == 0, 0, 1).to(device)\n",
    "            \n",
    "            # Get the BERT embeddings, output has shape (batch, tokens per batch, embedding_dim)\n",
    "            bert_embeddings = get_bert_embeddings(feature, segments_tensor, embedding_model).float()\n",
    "        \n",
    "            # For RNN\n",
    "            no_observations += target.shape[0]\n",
    "            model.batch_size = target.shape[0]\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            predictions = model(bert_embeddings).squeeze(1)\n",
    "\n",
    "            # save the predicted values from the last epoch\n",
    "            if epoch+1 == number_epoch:\n",
    "                pred_train = np.append(pred_train, predictions.detach().cpu().numpy(), axis = 0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = loss_fn(predictions, target)\n",
    "\n",
    "            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()*target.shape[0]     # Get the SSE of this batch from loss_fn()\n",
    "            epoch_sse += sse       # Get the SSE of this batch from our model_performance() function\n",
    "\n",
    "        valid_loss, valid_mse, __, __ = eval(dev_iter, model, embedding_model)\n",
    "\n",
    "        # print the result of this epoch\n",
    "        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
    "        print(f'| Epoch: {epoch+1:02} | Train Loss: {epoch_loss:.4f} | Train MSE: {epoch_mse:.4f} | Train RMSE: {epoch_mse**0.5:.4f} | \\\n",
    "        Val. Loss: {valid_loss:.4f} | Val. MSE: {valid_mse:.4f} |  Val. RMSE: {valid_mse**0.5:.4f} |')\n",
    "        \n",
    "        # step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        if valid_mse**0.5 < 0.536:\n",
    "            torch.save(model, '/content/drive/MyDrive/nlp_cw/approach1_model.pth')\n",
    "            break\n",
    "\n",
    "    return valid_loss, valid_mse**0.5\n",
    "\n",
    "        # print(f'| Epoch: {epoch+1:02} | Train Loss: {epoch_loss:.4f} | Train MSE: {epoch_mse:.4f} | Train RMSE: {epoch_mse**0.5:.4f} ')\n",
    "\n",
    "    # valid_loss, valid_mse, __, __ = eval(dev_iter, model, embedding_model)\n",
    "\n",
    "    # epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
    "    # print(f'| Epoch: {epoch+1:02} | Train Loss: {epoch_loss:.4f} | Train MSE: {epoch_mse:.4f} | Train RMSE: {epoch_mse**0.5:.4f} | \\\n",
    "    # Val. Loss: {valid_loss:.4f} | Val. MSE: {valid_mse:.4f} |  Val. RMSE: {valid_mse**0.5:.4f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on dev set\n",
    "\n",
    "def eval(data_iter, model, embedding_model):\n",
    "    \"\"\"\n",
    "    Evaluating model performance on the dev set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_sse = 0\n",
    "    pred_all = []\n",
    "    trg_all = []\n",
    "    no_observations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for feature, target in data_iter:\n",
    "\n",
    "            feature, target = feature.to(device), target.float().to(device)\n",
    "            segments_tensor = torch.where(feature == 0, 0, 1).to(device)\n",
    "\n",
    "            # Get the BERT embeddings\n",
    "            bert_embeddings = get_bert_embeddings(feature, segments_tensor, embedding_model).float()\n",
    "\n",
    "            # for RNN\n",
    "            no_observations = no_observations + target.shape[0]\n",
    "            model.batch_size = target.shape[0]\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            predictions = model(bert_embeddings).squeeze(1)\n",
    "\n",
    "            loss = loss_fn(predictions, target)\n",
    "\n",
    "            # We get the mse\n",
    "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
    "            sse, __ = model_performance(pred, trg)\n",
    "\n",
    "            epoch_loss += loss.item()*target.shape[0]    # Get the SSE of this batch from loss\n",
    "            epoch_sse += sse         # Get the SSE from our model_performance() function  \n",
    "            pred_all.extend(pred)\n",
    "            trg_all.extend(trg)\n",
    "\n",
    "    # Return the MSE from loss, MSE from our model_performance(), the predicted and target values\n",
    "    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model performance\n",
    "\n",
    "def model_performance(output, target, print_output=False):\n",
    "    \"\"\"\n",
    "    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
    "    \"\"\"\n",
    "\n",
    "    sq_error = (output - target)**2\n",
    "\n",
    "    sse = np.sum(sq_error)\n",
    "    mse = np.mean(sq_error)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    if print_output:\n",
    "        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n",
    "\n",
    "    return sse, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM without the embedding layer\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, linear_dim, num_layers, batch_size, device):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = num_layers, bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2label = nn.Linear(hidden_dim * 2, linear_dim)\n",
    "        self.linear = nn.Linear(linear_dim, 1)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
    "        return torch.zeros(2*num_layers, self.batch_size, self.hidden_dim).to(self.device), \\\n",
    "               torch.zeros(2*num_layers, self.batch_size, self.hidden_dim).to(self.device)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # Input sentence has shape (batch_size, seq_length, embedding_dim)\n",
    "        # Need to reshape before going into RNN\n",
    "        sentence = sentence.permute(1,0,2)\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            sentence, self.hidden)\n",
    "        \n",
    "        out = nn.functional.relu(self.hidden2label(lstm_out[-1]))\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing functions\n",
    "\n",
    "def get_editted_headline(input):\n",
    "      \"\"\" \n",
    "      replace the target word with the edit word to get the editted headline\n",
    "      \"\"\" \n",
    "      input[\"new\"] = input.apply(\n",
    "          lambda x: re.sub(r\"<.+/>\", x[\"edit\"], x[\"original\"]), axis = 1\n",
    "      ).str.strip().str.lower()\n",
    "      return input\n",
    "\n",
    "def remove_common_words(input):\n",
    "      \"\"\" \n",
    "      remove common words and punctuations in news headline\n",
    "      \"\"\" \n",
    "      remove_words = [\":\", \",\", \"-\", \"|\", \"?\", \"!\", \"#\", \"%\", \"$\", \"live updates\", \"latest updates\", \n",
    "                      \"bbc news\", \"us news\", \"world news\", \"fox news\", \"nbc news\",\n",
    "                      \"world news\", \"guardian us news\", \"news agency\", \"foxnews\", \n",
    "                      \"breitbart news\", \"ap news\", \"live news\"]\n",
    "      for word in remove_words:\n",
    "        input[\"new\"] = input[\"new\"].str.replace(word, \" \")\n",
    "      return input\n",
    "\n",
    "def drop_zero_grade(input):\n",
    "      \"\"\" \n",
    "      remove headlines that received no grade\n",
    "      \"\"\" \n",
    "      input = input.drop(input[input[\"meanGrade\"]==0].index)\n",
    "      input.index = range(len(input))\n",
    "      return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions for obtaining BERT hidden states\n",
    "\n",
    "def get_bert_tokens(text, tokenizer):\n",
    "      \"\"\" \n",
    "      input a list of texts and a tokenizer,\n",
    "      output the BERT token index\n",
    "      \"\"\"\n",
    "      tokens_tensor = tokenizer(text, padding = \"max_length\", truncation = True, \n",
    "                                max_length = 36, return_tensors = 'pt')[\"input_ids\"]\n",
    "\n",
    "      return tokens_tensor\n",
    "\n",
    "\n",
    "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
    "      \"\"\" \n",
    "      input BERT token index, segment tensor and the pre-trained BERT model,\n",
    "      output the BERT hidden states (12 layers)\n",
    "      \"\"\"\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "        output = model(tokens_tensor, segments_tensor)\n",
    "        hidden_states = output[2][-1]    # only takes the last hidden layer\n",
    "\n",
    "        # # Adjust for other hidden layers here\n",
    "        # Sum the last 4 hidden states\n",
    "        # hidden_states = torch.stack(output[2][-4:], dim = 0)\n",
    "        # hidden_states = torch.sum(hidden_states, dim = 0)\n",
    "\n",
    "        # Average the last 4 hidden states\n",
    "        # hidden_states = torch.stack(output[2][-4:], dim = 0)\n",
    "        # hidden_states = torch.mean(hidden_states, dim = 0)\n",
    "\n",
    "        # Sum all 12 hidden states\n",
    "        # hidden_states = torch.stack(output[2][:], dim = 0)\n",
    "        # hidden_states = torch.sum(hidden_states, dim = 0)\n",
    "\n",
    "        # Take the second last hidden state\n",
    "        # hidden_states = output[2][-2]\n",
    "\n",
    "        # Concatenate the last 4 hidden states\n",
    "        # Please multiply hidden_dim of Bi-LSTM by 4 as well.\n",
    "        # hidden_states = output[2][-1]\n",
    "        # for i in range(3):\n",
    "        #       hidden_states = torch.cat((hidden_states, output[2][-i-2]), dim = 3)\n",
    "\n",
    "      return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters here\n",
    "\n",
    "# Proportion of training data for train compared to dev\n",
    "train_proportion = 0.8\n",
    "\n",
    "# RNN hyperparameters\n",
    "num_layers = 1\n",
    "embedding_dim = 768   # Do not change, the BERT embedding dimension is fixed  \n",
    "hidden_dim = 300\n",
    "linear_dim = 150\n",
    "\n",
    "# Training hyperparameters\n",
    "epochs = 28\n",
    "batch_size = 32\n",
    "learning_rate = 0.00001\n",
    "milestones = [8, 20]\n",
    "gamma = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "train_df = get_editted_headline(train_df)\n",
    "train_df = remove_common_words(train_df)\n",
    "print(train_df.shape)\n",
    "train_df = drop_zero_grade(train_df)\n",
    "print(train_df.shape)\n",
    "\n",
    "dev_df = get_editted_headline(dev_df)\n",
    "dev_df = remove_common_words(dev_df)\n",
    "print(dev_df.shape)\n",
    "dev_df = drop_zero_grade(dev_df)\n",
    "print(dev_df.shape)\n",
    "\n",
    "# Create the bert model and tokenizer\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states = True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for optimising the network performance\n",
    "\n",
    "# hyperparameter tuning with random search\n",
    "\n",
    "# store the hyperparameters in lists\n",
    "lr = []\n",
    "batchsize = []\n",
    "hiddendim = []\n",
    "lineardim = []\n",
    "RMSEs = []\n",
    "\n",
    "# train n models by randomly sampling hyperparameters\n",
    "# after finding the best model, zoom into the neighbourhood of its hyperparameter space\n",
    "# and perform subsequent rounds of random search\n",
    "# lastly, hyperparameters like epoch are also tuned individually to achieve better validation performance\n",
    "\n",
    "num_model = 25\n",
    "\n",
    "for i in range(num_model): # sample the hyperparameters for each model\n",
    "\n",
    "        learning_rate = 10**(-2*np.random.rand()-2)  #10^-4 to 10^-2, sampling from log scale\n",
    "        lr.append(learning_rate)\n",
    "\n",
    "        batch = np.array([4,8,16,32,64,128])\n",
    "        batch_size = int(np.random.choice(batch))\n",
    "        batchsize.append(batch_size)\n",
    "\n",
    "        hid = np.array([20,40,60,80,100,120,140,160,180,200])\n",
    "        hidden = int(np.random.choice(hid))\n",
    "        hiddendim.append(hidden)\n",
    "\n",
    "        lin = np.array([20,30,40,50,60,70,80,90,100,120])\n",
    "        linear = int(np.random.choice(lin))\n",
    "        lineardim.append(linear)\n",
    "\n",
    "        # Convert preprocessed texts into a list\n",
    "        train_text = list(train_df[\"new\"])\n",
    "        train_tokens = get_bert_tokens(train_text, tokenizer)\n",
    "        print(train_tokens.shape)\n",
    "\n",
    "        # Split dataset\n",
    "        train_tokens = Task1Dataset(train_tokens, train_df[\"meanGrade\"])\n",
    "        train_split = round(len(train_text)*train_proportion)\n",
    "        val_split = len(train_text) - train_split\n",
    "        train_dataset, val_dataset = random_split(train_tokens, (train_split, val_split),\n",
    "                                                  generator = torch.Generator().manual_seed(SEED))\n",
    "        print(len(train_dataset))\n",
    "        print(len(val_dataset))\n",
    "\n",
    "        # Create the DataLoaders\n",
    "        train_loader = DataLoader(train_dataset, shuffle = True, batch_size = batch_size)\n",
    "        val_loader = DataLoader(val_dataset, shuffle = False, batch_size = batch_size)\n",
    "\n",
    "        # Create the model, loss function and optimizer\n",
    "        model = BiLSTM(embedding_dim, hidden, linear, num_layers, batch_size, device).to(device)\n",
    "        loss_fn = nn.MSELoss().to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = milestones, gamma = gamma)\n",
    "\n",
    "        # Start training\n",
    "        mse, rmse = train(train_loader, val_loader, model, bert, epochs, scheduler)\n",
    "\n",
    "        # save the result\n",
    "        RMSEs.append(rmse)\n",
    "\n",
    "\n",
    "# view results in pandas dataframe\n",
    "print(lr)\n",
    "print(batchsize)\n",
    "print(hiddendim)\n",
    "print(lineardim)\n",
    "print(RMSEs)\n",
    "\n",
    "result = {'lr': lr, 'batch_size': batchsize, 'hidden_dim': hiddendim, 'linear_dim': lineardim, 'RMSE': RMSEs}\n",
    "df = pd.DataFrame(result)\n",
    "df = df.sort_values(by = ['RMSEs'], ascending = True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the final model\n",
    "\n",
    "# Convert preprocessed texts into a list\n",
    "train_text = list(train_df[\"new\"])\n",
    "train_tokens = get_bert_tokens(train_text, tokenizer)\n",
    "print(train_tokens.shape)\n",
    "\n",
    "dev_text = list(dev_df[\"new\"])\n",
    "dev_tokens = get_bert_tokens(dev_text, tokenizer)\n",
    "print(dev_tokens.shape)\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "train_tokens = Task1Dataset(train_tokens, train_df[\"meanGrade\"])\n",
    "dev_tokens = Task1Dataset(dev_tokens, dev_df[\"meanGrade\"])\n",
    "\n",
    "## Split training set into a train and validation set here for preliminary experimentations\n",
    "\n",
    "# train_split = round(len(train_text)*train_proportion)\n",
    "# val_split = len(train_text) - train_split\n",
    "# train_dataset, val_dataset = random_split(train_tokens, (train_split, val_split),\n",
    "#                                           generator = torch.Generator().manual_seed(SEED))\n",
    "print(len(train_tokens))\n",
    "print(len(dev_tokens))\n",
    "\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_loader = DataLoader(train_tokens, shuffle = True, batch_size = batch_size)\n",
    "val_loader = DataLoader(dev_tokens, shuffle = False, batch_size = batch_size)\n",
    "\n",
    "\n",
    "# Create the model, loss function and optimizer\n",
    "model = BiLSTM(embedding_dim, hidden_dim, linear_dim, num_layers, batch_size, device).to(device)\n",
    "loss_fn = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = milestones, gamma = gamma)\n",
    "\n",
    "\n",
    "# Start training\n",
    "train(train_loader, val_loader, model, bert, epochs, scheduler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test RMSE\n",
    "\n",
    "# dataloading and preprocessing\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "print(test_df.shape)\n",
    "test_df = get_editted_headline(test_df)\n",
    "test_df = remove_common_words(test_df)\n",
    "test_df = drop_zero_grade(test_df)\n",
    "print(test_df.shape)\n",
    "\n",
    "# get BERT tokens\n",
    "test_text = list(test_df[\"new\"])\n",
    "test_tokens = get_bert_tokens(test_text, tokenizer)\n",
    "print(test_tokens.shape)\n",
    "\n",
    "# create the test loader\n",
    "test_tokens = Task1Dataset(test_tokens, test_df[\"meanGrade\"])\n",
    "print(len(test_tokens))\n",
    "test_loader = DataLoader(test_tokens, shuffle = False, batch_size = batch_size)\n",
    "\n",
    "# evaluation\n",
    "test_loss, test_MSE, __, __ = eval(test_loader, model, bert)\n",
    "print(f\"The final RMSE on the test set is : {test_MSE**0.5:.4f}\")\n",
    "\n",
    "\n",
    "# get the evaluation for each quintile of meanGrade\n",
    "test_df[\"quintile\"] = pd.qcut(test_df[\"meanGrade\"], 5, labels = False)\n",
    "\n",
    "for i in range(5):\n",
    "    test_quintile = test_df[test_df[\"quintile\"] == i]\n",
    "    test_quintile.index = range(len(test_quintile))\n",
    "\n",
    "    test_text = list(test_quintile[\"new\"])\n",
    "    test_tokens = get_bert_tokens(test_text, tokenizer)\n",
    "    print(test_tokens.shape)\n",
    "\n",
    "    # create the test loader\n",
    "    test_tokens = Task1Dataset(test_tokens, test_quintile[\"meanGrade\"])\n",
    "    print(len(test_tokens))\n",
    "    test_loader = DataLoader(test_tokens, shuffle = False, batch_size = batch_size)\n",
    "\n",
    "    # evaluation\n",
    "    test_loss, test_MSE, __, __ = eval(test_loader, model, bert)\n",
    "    print(f\"The final RMSE on the {i+1} quintile of the test set is : {test_MSE**0.5:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-contextualised word embeddings\n",
    "\n",
    "# Define training loop\n",
    "def train(train_iter, dev_iter, model, number_epoch):\n",
    "    \"\"\"\n",
    "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    print(\"Training model.\")\n",
    "\n",
    "    for epoch in range(1, number_epoch+1):\n",
    "\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_sse = 0\n",
    "        no_observations = 0  # Observations used for training so far\n",
    "\n",
    "        for batch in train_iter:\n",
    "\n",
    "            feature, target = batch\n",
    "\n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "            # for RNN:\n",
    "            model.batch_size = target.shape[0]\n",
    "            no_observations = no_observations + target.shape[0]\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            predictions = model(feature).squeeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = loss_fn(predictions, target)\n",
    "\n",
    "            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()*target.shape[0]\n",
    "            epoch_sse += sse\n",
    "\n",
    "        valid_loss, valid_mse, __, __ = eval(dev_iter, model)\n",
    "\n",
    "        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
    "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n",
    "        Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} |')\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate performance on dev set\n",
    "def eval(data_iter, model):\n",
    "    \"\"\"\n",
    "    Evaluating model performance on the dev set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_sse = 0\n",
    "    pred_all = []\n",
    "    trg_all = []\n",
    "    no_observations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_iter:\n",
    "            feature, target = batch\n",
    "\n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "            # for RNN:\n",
    "            model.batch_size = target.shape[0]\n",
    "            no_observations = no_observations + target.shape[0]\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            predictions = model(feature).squeeze(1)\n",
    "            loss = loss_fn(predictions, target)\n",
    "\n",
    "            # We get the mse\n",
    "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
    "            sse, __ = model_performance(pred, trg)\n",
    "\n",
    "            epoch_loss += loss.item()*target.shape[0]\n",
    "            epoch_sse += sse\n",
    "            pred_all.extend(pred)\n",
    "            trg_all.extend(trg)\n",
    "\n",
    "    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)\n",
    "\n",
    "\n",
    "\n",
    "# How print the model performance\n",
    "def model_performance(output, target, print_output=False):\n",
    "    \"\"\"\n",
    "    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
    "    \"\"\"\n",
    "\n",
    "    sq_error = (output - target)**2\n",
    "\n",
    "    sse = np.sum(sq_error)\n",
    "    mse = np.mean(sq_error)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    if print_output:\n",
    "        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n",
    "\n",
    "    return sse, mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-contextualised word embeddings\n",
    "\n",
    "def create_vocab(data):\n",
    "    \"\"\"\n",
    "    Creating a corpus of all the tokens used\n",
    "    \"\"\"\n",
    "    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
    "\n",
    "    for sentence in data:\n",
    "\n",
    "        tokenized_sentence = []\n",
    "\n",
    "        for token in sentence.split(' '): # simplest split is\n",
    "\n",
    "            tokenized_sentence.append(token)\n",
    "\n",
    "        tokenized_corpus.append(tokenized_sentence)\n",
    "\n",
    "    # Create single list of all vocabulary\n",
    "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
    "\n",
    "    for sentence in tokenized_corpus:\n",
    "\n",
    "        for token in sentence:\n",
    "\n",
    "            if token not in vocabulary:\n",
    "\n",
    "                if True:\n",
    "                    vocabulary.append(token)\n",
    "\n",
    "    return vocabulary, tokenized_corpus\n",
    "\n",
    "\n",
    "def collate_fn_padd(batch):\n",
    "    '''\n",
    "    We add padding to our minibatches and create tensors for our model\n",
    "    '''\n",
    "\n",
    "    batch_labels = [l for f, l in batch]\n",
    "    batch_features = [f for f, l in batch]\n",
    "\n",
    "    batch_features_len = [len(f) for f, l in batch]\n",
    "\n",
    "    seq_tensor = torch.zeros((len(batch), max(batch_features_len))).long()\n",
    "\n",
    "    for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\n",
    "        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "\n",
    "    batch_labels = torch.FloatTensor(batch_labels)\n",
    "\n",
    "    return seq_tensor, batch_labels\n",
    "\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, batch_size, device):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2label = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
    "        return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device), \\\n",
    "               torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embedded = self.embedding(sentence)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embedded.view(len(embedded), self.batch_size, self.embedding_dim), self.hidden)\n",
    "\n",
    "        out = self.hidden2label(lstm_out[-1])\n",
    "        return out\n",
    "  \n",
    "\n",
    "\n",
    "## Approach 1 code, using functions defined above:\n",
    "\n",
    "# We set our training data and test data\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_df = get_editted_headline(test_df)\n",
    "test_df = remove_common_words(test_df)\n",
    "test_df = drop_zero_grade(test_df)\n",
    "\n",
    "train_df = get_editted_headline(train_df)\n",
    "train_df = remove_common_words(train_df)\n",
    "train_df = drop_zero_grade(train_df)\n",
    "\n",
    "training_data = train_df['new']\n",
    "test_data = test_df['new']\n",
    "\n",
    "# Creating word vectors\n",
    "training_vocab, training_tokenized_corpus = create_vocab(training_data)\n",
    "test_vocab, test_tokenized_corpus = create_vocab(test_data)\n",
    "\n",
    "# Creating joint vocab from test and train:\n",
    "joint_vocab, joint_tokenized_corpus = create_vocab(pd.concat([training_data, test_data]))\n",
    "\n",
    "print(\"Vocab created.\")\n",
    "\n",
    "# We create representations for our tokens\n",
    "wvecs = [] # word vectors\n",
    "word2idx = [] # word2index\n",
    "idx2word = []\n",
    "\n",
    "# This is a large file, it will take a while to load in the memory!\n",
    "with codecs.open('glove.6B.100d.txt', 'r','utf-8') as f:\n",
    "  index = 1\n",
    "  for line in f.readlines():\n",
    "    # Ignore the first line - first line typically contains vocab, dimensionality\n",
    "    if len(line.strip().split()) > 3:\n",
    "      word = line.strip().split()[0]\n",
    "      if word in joint_vocab:\n",
    "          (word, vec) = (word,\n",
    "                     list(map(float,line.strip().split()[1:])))\n",
    "          wvecs.append(vec)\n",
    "          word2idx.append((word, index))\n",
    "          idx2word.append((index, word))\n",
    "          index += 1\n",
    "\n",
    "wvecs = np.array(wvecs)\n",
    "word2idx = dict(word2idx)\n",
    "idx2word = dict(idx2word)\n",
    "\n",
    "vectorized_seqs = [[word2idx[tok] for tok in seq if tok in word2idx] for seq in training_tokenized_corpus]\n",
    "\n",
    "# To avoid any sentences being empty (if no words match to our word embeddings)\n",
    "vectorized_seqs = [x if len(x) > 0 else [0] for x in vectorized_seqs]\n",
    "\n",
    "INPUT_DIM = len(word2idx)\n",
    "EMBEDDING_DIM = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model = BiLSTM(EMBEDDING_DIM, 50, INPUT_DIM, BATCH_SIZE, device)\n",
    "print(\"Model initialised.\")\n",
    "\n",
    "model.to(device)\n",
    "# We provide the model with our embeddings\n",
    "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))\n",
    "\n",
    "feature = vectorized_seqs\n",
    "\n",
    "# 'feature' is a list of lists, each containing embedding IDs for word tokens\n",
    "train_and_dev = Task1Dataset(feature, train_df['meanGrade'])\n",
    "\n",
    "train_examples = round(len(train_and_dev)*train_proportion)\n",
    "dev_examples = len(train_and_dev) - train_examples\n",
    "\n",
    "train_dataset, dev_dataset = random_split(train_and_dev,\n",
    "                                           (train_examples,\n",
    "                                            dev_examples))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
    "\n",
    "print(\"Dataloaders created.\")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train(train_loader, dev_loader, model, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune a pre-trained BERT\n",
    "\n",
    "class BertRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, freeze_bert = False):\n",
    "        super(BertRegressor, self).__init__()\n",
    "\n",
    "        input, hidden, output = 768, 120, 1\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(input, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden, output)\n",
    "        )\n",
    "\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input, segment_tensor):\n",
    "        output = self.bert(input, segment_tensor)\n",
    "\n",
    "        last_hidden_state = output[0][:, 0, :]\n",
    "\n",
    "        out = self.regressor(last_hidden_state)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertRegressor().to(device)\n",
    "# Convert preprocessed texts into a list\n",
    "train_text = list(train_df[\"new\"])\n",
    "train_tokens = get_bert_tokens(train_text, tokenizer)\n",
    "print(train_tokens.shape)\n",
    "\n",
    "dev_text = list(dev_df[\"new\"])\n",
    "dev_tokens = get_bert_tokens(dev_text, tokenizer)\n",
    "print(dev_tokens.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "train_tokens = Task1Dataset(train_tokens, train_df[\"meanGrade\"])\n",
    "dev_tokens = Task1Dataset(dev_tokens, dev_df[\"meanGrade\"])\n",
    "# train_split = round(len(train_text)*train_proportion)\n",
    "# val_split = len(train_text) - train_split\n",
    "# train_dataset, val_dataset = random_split(train_tokens, (train_split, val_split),\n",
    "#                                           generator = torch.Generator().manual_seed(SEED))\n",
    "print(len(train_tokens))\n",
    "print(len(dev_tokens))\n",
    "\n",
    "# Change the number of epochs here\n",
    "epochs = 8\n",
    "\n",
    "loss_fn = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_loader = DataLoader(train_tokens, shuffle = True, batch_size = batch_size)\n",
    "val_loader = DataLoader(dev_tokens, shuffle = False, batch_size = batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_sse = 0\n",
    "    no_observations = 0\n",
    "\n",
    "    for feature, target in train_loader:\n",
    "\n",
    "        feature, target = feature.to(device), target.float().to(device)\n",
    "        no_observations += target.shape[0]\n",
    "        segments_tensor = torch.where(feature == 0, 0, 1).to(device)\n",
    "        out = model(feature, segments_tensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        target = target.reshape((target.shape[0],1))\n",
    "        loss = loss_fn(out, target).float()\n",
    "        # print(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()*target.shape[0]     # Get the SSE of this batch from loss_fn()\n",
    "\n",
    "    # print the result of this epoch\n",
    "    epoch_loss = epoch_loss / no_observations\n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {epoch_loss:.4f} | Train RMSE: {epoch_loss**0.5:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use no pre-trained representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_dev = train_df['edit']\n",
    "\n",
    "training_data, dev_data, training_y, dev_y = train_test_split(train_df['edit'], train_df['meanGrade'],\n",
    "                                                                        test_size=(1-train_proportion),\n",
    "                                                                        random_state=42)\n",
    "\n",
    "# We train a Tf-idf model\n",
    "count_vect = CountVectorizer(stop_words='english')\n",
    "train_counts = count_vect.fit_transform(training_data)\n",
    "transformer = TfidfTransformer().fit(train_counts)\n",
    "train_counts = transformer.transform(train_counts)\n",
    "regression_model = LinearRegression().fit(train_counts, training_y)\n",
    "\n",
    "# Train predictions\n",
    "predicted_train = regression_model.predict(train_counts)\n",
    "\n",
    "# Calculate Tf-idf using train and dev, and validate model on dev:\n",
    "test_and_test_counts = count_vect.transform(train_and_dev)\n",
    "transformer = TfidfTransformer().fit(test_and_test_counts)\n",
    "\n",
    "test_counts = count_vect.transform(dev_data)\n",
    "\n",
    "test_counts = transformer.transform(test_counts)\n",
    "\n",
    "# Dev predictions\n",
    "predicted = regression_model.predict(test_counts)\n",
    "\n",
    "# We run the evaluation:\n",
    "print(\"\\nTrain performance:\")\n",
    "sse, mse = model_performance(predicted_train, training_y, True)\n",
    "\n",
    "print(\"\\nDev performance:\")\n",
    "sse, mse = model_performance(predicted, dev_y, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline for the task\n",
    "pred_baseline = torch.zeros(len(dev_y)) + np.mean(training_y)\n",
    "print(\"\\nBaseline performance:\")\n",
    "sse, mse = model_performance(pred_baseline, dev_y, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data and replace the old words with editted words\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "dev_df = pd.read_csv(\"dev.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "train_df = get_editted_headline(train_df)\n",
    "dev_df = get_editted_headline(dev_df)\n",
    "test_df = get_editted_headline(test_df)\n",
    "train_df = remove_common_words(train_df)\n",
    "dev_df = remove_common_words(dev_df)\n",
    "test_df = remove_common_words(test_df)\n",
    "train_df = drop_zero_grade(train_df)\n",
    "dev_df = drop_zero_grade(dev_df)\n",
    "test_df = drop_zero_grade(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding start and stop tokens to each sentence, and tokenize these sentence\n",
    "def get_tokenized_corpus(data):\n",
    "  tokenized_corpus = []\n",
    "  for sentence in data:\n",
    "    sentence = \"<s> \" + sentence + \" </s>\"\n",
    "    tokenized_sentence = []\n",
    "    for token in sentence.split(' '):\n",
    "      tokenized_sentence.append(token)\n",
    "    tokenized_corpus.append(tokenized_sentence)\n",
    "  return tokenized_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = get_tokenized_corpus(train_df[\"new\"])\n",
    "tokenized_dev = get_tokenized_corpus(dev_df[\"new\"])\n",
    "tokenized_test = get_tokenized_corpus(test_df[\"new\"])\n",
    "print(tokenized_train)\n",
    "print(tokenized_dev)\n",
    "print(tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec vocabulary, which is built through training data and used to convert words to correponding indexs or indexs to corresponding.\n",
    "class Vocabulary(object):\n",
    "  def __init__(self):\n",
    "    self._word2idx = {}\n",
    "    self.idx2word = []\n",
    "    # 0-padding token\n",
    "    self.add_word('<pad>')\n",
    "    # Unknown words\n",
    "    self.add_word('<unk>')\n",
    "    self._unk_idx = self._word2idx['<unk>']\n",
    "\n",
    "  def word2idx(self, word):\n",
    "    return self._word2idx.get(word, self._unk_idx)\n",
    "\n",
    "  def add_word(self, word):\n",
    "    if word not in self._word2idx:\n",
    "      self.idx2word.append(word)\n",
    "      self._word2idx[word] = len(self.idx2word) - 1\n",
    "\n",
    "  def build_from_data(self, data):\n",
    "    for sentence in data:\n",
    "      for word in sentence:\n",
    "        self.add_word(word)\n",
    "\n",
    "  def convert_idxs_to_words(self, idxs):\n",
    "    return ' '.join(self.idx2word[idx] for idx in idxs)\n",
    "\n",
    "  def convert_words_to_idxs(self, words):\n",
    "    return [self.word2idx(w) for w in words]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.idx2word)\n",
    "  \n",
    "  def __repr__(self):\n",
    "    return \"Vocabulary with {} items\".format(self.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary()\n",
    "print(vocab.idx2word)\n",
    "vocab.build_from_data(tokenized_train)\n",
    "print(vocab.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert data to indexs through vocabulary\n",
    "def convert_data_to_idx(data):\n",
    "  corpus_idxs = []\n",
    "  for sentence in data:\n",
    "    corpus_idxs.append(vocab.convert_words_to_idxs(sentence))\n",
    "  return corpus_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs = convert_data_to_idx(tokenized_train)\n",
    "test_idxs = convert_data_to_idx(tokenized_test)\n",
    "dev_idxs = convert_data_to_idx(tokenized_dev)\n",
    "print(train_idxs)\n",
    "print(test_idxs)\n",
    "print(dev_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad the sentences in training data, validation data and testing data to their corresponding maximum lenghth.\n",
    "def collate_fn_padd(data):\n",
    "  lens = []\n",
    "  for sentence in data:\n",
    "    lens.append(len(sentence))\n",
    "  \n",
    "  padded = torch.zeros((len(data), max(lens)))\n",
    "\n",
    "  for i in range(len(data)):\n",
    "    padded[i][: lens[i]] = torch.LongTensor(data[i])\n",
    "\n",
    "  return padded.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs_padded = collate_fn_padd(train_idxs).to(device)\n",
    "test_idxs_padded = collate_fn_padd(test_idxs).to(device)\n",
    "dev_idxs_padded = collate_fn_padd(dev_idxs).to(device)\n",
    "\n",
    "print(train_idxs_padded.size())\n",
    "print(test_idxs_padded.size())\n",
    "print(dev_idxs_padded.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural network with embedding layer\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):  \n",
    "        super(FFNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        embedded = self.embedding(x)\n",
    "        #Average through each words in one sentence\n",
    "        sent_lens = x.ne(0).sum(1, keepdims=True)\n",
    "        sent_lens = x.ne(0).sum(1, keepdims=True)\n",
    "        averaged = embedded.sum(1) / sent_lens\n",
    "        out = self.fc1(averaged)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "\n",
    "def train(train_iter, dev_iter, model, number_epoch, optimizer, loss_fn):\n",
    "    \"\"\"\n",
    "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Training model.\")\n",
    "    \n",
    "    for epoch in range(number_epoch):\n",
    "        \n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_sse = 0\n",
    "        no_observations = 0    # Observations used for training so far\n",
    "        pred_train = np.empty(0)\n",
    "\n",
    "        for feature, target in train_iter:\n",
    "\n",
    "            feature, target = feature.to(device).long(), target.float().to(device)\n",
    "            \n",
    "            no_observations += target.shape[0]\n",
    "\n",
    "            predictions = model(feature).squeeze(1)\n",
    "\n",
    "            # save the predicted values from the last epoch\n",
    "            if epoch+1 == number_epoch:\n",
    "                pred_train = np.append(pred_train, predictions.detach().cpu().numpy(), axis = 0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = loss_fn(predictions, target)\n",
    "\n",
    "            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()*target.shape[0]     # Get the SSE of this batch from loss_fn()\n",
    "            epoch_sse += sse       # Get the SSE of this batch from our model_performance() function\n",
    "\n",
    "        valid_loss, valid_mse, __, __ = eval(dev_iter, model, loss_fn)\n",
    "\n",
    "        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
    "        print(f'| Epoch: {epoch+1:02} | Train Loss: {epoch_loss:.4f} | Train MSE: {epoch_mse:.4f} | Train RMSE: {epoch_mse**0.5:.4f} | \\\n",
    "        Val. Loss: {valid_loss:.4f} | Val. MSE: {valid_mse:.4f} |  Val. RMSE: {valid_mse**0.5:.4f} |')\n",
    "        \n",
    "\n",
    "        if valid_mse**0.5 < 0.540:\n",
    "            torch.save(model, '/content/drive/MyDrive/nlp_cw/approach1_model.pth')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on dev set\n",
    "\n",
    "def eval(data_iter, model, loss_fn):\n",
    "    \"\"\"\n",
    "    Evaluating model performance on the dev set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_sse = 0\n",
    "    pred_all = []\n",
    "    trg_all = []\n",
    "    no_observations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for feature, target in data_iter:\n",
    "\n",
    "            feature, target = feature.to(device).long(), target.float().to(device)\n",
    "\n",
    "\n",
    "            no_observations = no_observations + target.shape[0]\n",
    "\n",
    "            predictions = model(feature).squeeze(1)\n",
    "\n",
    "            loss = loss_fn(predictions, target)\n",
    "\n",
    "            # We get the mse\n",
    "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
    "            sse, __ = model_performance(pred, trg)\n",
    "\n",
    "            epoch_loss += loss.item()*target.shape[0]    # Get the SSE of this batch from loss\n",
    "            epoch_sse += sse         # Get the SSE from our model_performance() function  \n",
    "            pred_all.extend(pred)\n",
    "            trg_all.extend(trg)\n",
    "\n",
    "    # Return the MSE from loss, MSE from our model_performance(), the predicted and target values\n",
    "    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 13\n",
    "embedding_dim = len(vocab.idx2word) \n",
    "hidden_dim = 20\n",
    "learning_rate = 0.00002\n",
    "\n",
    "# Create dataset\n",
    "train_tokens = Task1Dataset(train_idxs_padded, train_df[\"meanGrade\"])\n",
    "dev_tokens = Task1Dataset(dev_idxs_padded, dev_df[\"meanGrade\"])\n",
    "\n",
    "print(len(train_tokens))\n",
    "print(len(dev_tokens))\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_loader = DataLoader(train_tokens, shuffle = True, batch_size = batch_size)\n",
    "val_loader = DataLoader(dev_tokens, shuffle = False, batch_size = batch_size)\n",
    "\n",
    "# Create the model, loss function and optimizer\n",
    "model = FFNN(embedding_dim, hidden_dim, len(vocab.idx2word)).to(device)\n",
    "loss_fn = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Start training\n",
    "train(train_loader, val_loader, model, epochs, optimizer, loss_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test RMSE\n",
    "\n",
    "test_tokens = Task1Dataset(test_idxs_padded, test_df[\"meanGrade\"])\n",
    "\n",
    "print(len(test_tokens))\n",
    "test_loader = DataLoader(test_tokens, shuffle = False, batch_size = batch_size)\n",
    "\n",
    "# evaluation\n",
    "test_loss, test_MSE, __, __ = eval(test_loader, model, loss_fn)\n",
    "print(f'| MSE: {test_MSE:.2f} | RMSE: {test_MSE**0.5:.2f} |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN model\n",
    "class CNN(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, out_channels, window_size, dropout):\n",
    "    super(CNN, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "    self.conv = nn.Conv2d(\n",
    "      in_channels=1, out_channels=out_channels,\n",
    "      kernel_size=(window_size, embedding_dim))\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.fc = nn.Linear(out_channels, 1)\n",
    "        \n",
    "  def forward(self, x):\n",
    "    embedded = self.embedding(x)\n",
    "    embedded = embedded.unsqueeze(1)\n",
    "    feature_maps = self.conv(embedded)\n",
    "    feature_maps = feature_maps.squeeze(3)\n",
    "    feature_maps = nn.functional.relu(feature_maps)\n",
    "    pooled = nn.functional.max_pool1d(feature_maps, feature_maps.shape[2])\n",
    "    pooled = pooled.squeeze(2)\n",
    "    dropped = self.dropout(pooled)\n",
    "    preds = self.fc(dropped)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 15\n",
    "embedding_dim = len(vocab.idx2word) \n",
    "hidden_dim = 15\n",
    "learning_rate = 0.00003\n",
    "N_OUT_CHANNELS = 100\n",
    "WINDOW_SIZE = 1\n",
    "DROPOUT = 0.6\n",
    "\n",
    "# Create dataset\n",
    "train_tokens = Task1Dataset(train_idxs_padded, train_df[\"meanGrade\"])\n",
    "dev_tokens = Task1Dataset(dev_idxs_padded, dev_df[\"meanGrade\"])\n",
    "\n",
    "print(len(train_tokens))\n",
    "print(len(dev_tokens))\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_loader = DataLoader(train_tokens, shuffle = True, batch_size = batch_size)\n",
    "val_loader = DataLoader(dev_tokens, shuffle = False, batch_size = batch_size)\n",
    "\n",
    "# Create the model, loss function and optimizer\n",
    "model = CNN(len(vocab.idx2word), embedding_dim, N_OUT_CHANNELS, WINDOW_SIZE, DROPOUT).to(device)\n",
    "loss_fn = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Start training\n",
    "train(train_loader, val_loader, model, epochs, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test RMSE\n",
    "\n",
    "test_tokens = Task1Dataset(test_idxs_padded, test_df[\"meanGrade\"])\n",
    "\n",
    "print(len(test_tokens))\n",
    "test_loader = DataLoader(test_tokens, shuffle = False, batch_size = batch_size)\n",
    "\n",
    "# evaluation\n",
    "test_loss, test_MSE, __, __ = eval(test_loader, model, loss_fn)\n",
    "print(f'| MSE: {test_MSE:.2f} | RMSE: {test_MSE**0.5:.2f} |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 8\n",
    "embedding_dim = len(vocab.idx2word) \n",
    "hidden_dim = 15\n",
    "learning_rate = 0.00001\n",
    "N_OUT_CHANNELS = 100\n",
    "WINDOW_SIZE = 2\n",
    "DROPOUT = 0.4\n",
    "\n",
    "# Create dataset\n",
    "train_tokens = Task1Dataset(train_idxs_padded, train_df[\"meanGrade\"])\n",
    "dev_tokens = Task1Dataset(dev_idxs_padded, dev_df[\"meanGrade\"])\n",
    "\n",
    "print(len(train_tokens))\n",
    "print(len(dev_tokens))\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_loader = DataLoader(train_tokens, shuffle = True, batch_size = batch_size)\n",
    "val_loader = DataLoader(dev_tokens, shuffle = False, batch_size = batch_size)\n",
    "\n",
    "# Create the model, loss function and optimizer\n",
    "model = CNN(len(vocab.idx2word), embedding_dim, N_OUT_CHANNELS, WINDOW_SIZE, DROPOUT).to(device)\n",
    "loss_fn = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Start training\n",
    "train(train_loader, val_loader, model, epochs, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test RMSE\n",
    "\n",
    "test_tokens = Task1Dataset(test_idxs_padded, test_df[\"meanGrade\"])\n",
    "\n",
    "print(len(test_tokens))\n",
    "test_loader = DataLoader(test_tokens, shuffle = False, batch_size = batch_size)\n",
    "\n",
    "# evaluation\n",
    "test_loss, test_MSE, __, __ = eval(test_loader, model, loss_fn)\n",
    "print(f'| MSE: {test_MSE:.2f} | RMSE: {test_MSE**0.5:.2f} |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bidirectional LSTM model\n",
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, batch_size, device):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2label = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
    "        return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device), \\\n",
    "               torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embedded = self.embedding(sentence)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embedded.view(len(embedded), self.batch_size, self.embedding_dim), self.hidden)\n",
    "\n",
    "        out = self.hidden2label(lstm_out[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "\n",
    "def train(train_iter, dev_iter, model, number_epoch, optimizer, loss_fn):\n",
    "    \"\"\"\n",
    "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Training model.\")\n",
    "\n",
    "    for epoch in range(number_epoch):\n",
    "        \n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_sse = 0\n",
    "        no_observations = 0    # Observations used for training so far\n",
    "        pred_train = np.empty(0)\n",
    "\n",
    "        for feature, target in train_iter:\n",
    "\n",
    "            feature, target = feature.to(device).long(), target.float().to(device)\n",
    "            \n",
    "            no_observations += target.shape[0]\n",
    "            model.batch_size = target.shape[0]\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            predictions = model(feature).squeeze(1)\n",
    "\n",
    "            # save the predicted values from the last epoch\n",
    "            if epoch+1 == number_epoch:\n",
    "                pred_train = np.append(pred_train, predictions.detach().cpu().numpy(), axis = 0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = loss_fn(predictions, target)\n",
    "\n",
    "            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()*target.shape[0]     # Get the SSE of this batch from loss_fn()\n",
    "            epoch_sse += sse       # Get the SSE of this batch from our model_performance() function\n",
    "\n",
    "        valid_loss, valid_mse, __, __ = eval(dev_iter, model, loss_fn)\n",
    "\n",
    "        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
    "        print(f'| Epoch: {epoch+1:02} | Train Loss: {epoch_loss:.4f} | Train MSE: {epoch_mse:.4f} | Train RMSE: {epoch_mse**0.5:.4f} | \\\n",
    "        Val. Loss: {valid_loss:.4f} | Val. MSE: {valid_mse:.4f} |  Val. RMSE: {valid_mse**0.5:.4f} |')\n",
    "        \n",
    "\n",
    "        if valid_mse**0.5 < 0.540:\n",
    "            torch.save(model, '/content/drive/MyDrive/nlp_cw/approach1_model.pth')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on dev set\n",
    "\n",
    "def eval(data_iter, model, loss_fn):\n",
    "    \"\"\"\n",
    "    Evaluating model performance on the dev set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_sse = 0\n",
    "    pred_all = []\n",
    "    trg_all = []\n",
    "    no_observations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for feature, target in data_iter:\n",
    "\n",
    "            feature, target = feature.to(device).long(), target.float().to(device)\n",
    "\n",
    "\n",
    "            no_observations = no_observations + target.shape[0]\n",
    "            model.batch_size = target.shape[0]\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            predictions = model(feature).squeeze(1)\n",
    "\n",
    "            loss = loss_fn(predictions, target)\n",
    "\n",
    "            # We get the mse\n",
    "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
    "            sse, __ = model_performance(pred, trg)\n",
    "\n",
    "            epoch_loss += loss.item()*target.shape[0]    # Get the SSE of this batch from loss\n",
    "            epoch_sse += sse         # Get the SSE from our model_performance() function  \n",
    "            pred_all.extend(pred)\n",
    "            trg_all.extend(trg)\n",
    "\n",
    "    # Return the MSE from loss, MSE from our model_performance(), the predicted and target values\n",
    "    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "embedding_dim = len(vocab.idx2word) \n",
    "hidden_dim = 10\n",
    "learning_rate = 0.00008\n",
    "\n",
    "# Create dataset\n",
    "train_tokens = Task1Dataset(train_idxs_padded, train_df[\"meanGrade\"])\n",
    "dev_tokens = Task1Dataset(dev_idxs_padded, dev_df[\"meanGrade\"])\n",
    "\n",
    "print(len(train_tokens))\n",
    "print(len(dev_tokens))\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_loader = DataLoader(train_tokens, shuffle = True, batch_size = batch_size)\n",
    "val_loader = DataLoader(dev_tokens, shuffle = False, batch_size = batch_size)\n",
    "\n",
    "# Create the model, loss function and optimizer\n",
    "model = BiLSTM(embedding_dim, hidden_dim, len(vocab.idx2word), batch_size, device).to(device)\n",
    "loss_fn = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Start training\n",
    "train(train_loader, val_loader, model, epochs, optimizer, loss_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test RMSE\n",
    "\n",
    "test_tokens = Task1Dataset(test_idxs_padded, test_df[\"meanGrade\"])\n",
    "\n",
    "print(len(test_tokens))\n",
    "test_loader = DataLoader(test_tokens, shuffle = False, batch_size = batch_size)\n",
    "\n",
    "# evaluation\n",
    "test_loss, test_MSE, __, __ = eval(test_loader, model, loss_fn)\n",
    "print(f'| MSE: {test_MSE:.2f} | RMSE: {test_MSE**0.5:.2f} |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the sentence to the maximum lengths in training, validation and testing data\n",
    "def collate_fn_padd(data):\n",
    "  lens = []\n",
    "  for sentence in data:\n",
    "    lens.append(len(sentence))\n",
    "  \n",
    "  padded = torch.zeros((len(data), 31))\n",
    "\n",
    "  for i in range(len(data)):\n",
    "    padded[i][: lens[i]] = torch.LongTensor(data[i])\n",
    "\n",
    "  return padded.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs_padded = collate_fn_padd(train_idxs).to(device)\n",
    "test_idxs_padded = collate_fn_padd(test_idxs).to(device)\n",
    "dev_idxs_padded = collate_fn_padd(dev_idxs).to(device)\n",
    "\n",
    "print(train_idxs_padded.size())\n",
    "print(test_idxs_padded.size())\n",
    "print(dev_idxs_padded.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression model\n",
    "regression_model = LinearRegression().fit(train_idxs_padded.cpu().numpy(), train_df[\"meanGrade\"])\n",
    "\n",
    "# Train predictions\n",
    "predicted_train = regression_model.predict(train_idxs_padded.cpu().numpy())\n",
    "\n",
    "\n",
    "predicted = regression_model.predict(dev_idxs_padded.cpu().numpy())\n",
    "\n",
    "# We run the evaluation:\n",
    "print(\"\\nTrain performance:\")\n",
    "sse, mse = model_performance(predicted_train, train_df[\"meanGrade\"], True)\n",
    "\n",
    "print(\"\\nDev performance:\")\n",
    "sse, mse = model_performance(predicted, dev_df[\"meanGrade\"], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test RMSE\n",
    "predicted_test = regression_model.predict(test_idxs_padded.cpu().numpy())\n",
    "\n",
    "print(len(predicted_test))\n",
    "\n",
    "# evaluation\n",
    "print(\"\\nTest performance:\")\n",
    "sse, mse = model_performance(predicted_test, test_df[\"meanGrade\"], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest regression model\n",
    "rfc = RandomForestRegressor(n_estimators=30, max_depth=5)\n",
    "rfc = rfc.fit(train_idxs_padded.cpu().numpy(), train_df[\"meanGrade\"])\n",
    "predicted_train = rfc.predict(train_idxs_padded.cpu().numpy())\n",
    "predicted = rfc.predict(dev_idxs_padded.cpu().numpy())\n",
    "\n",
    "# We run the evaluation:\n",
    "print(\"\\nTrain performance:\")\n",
    "sse, mse = model_performance(predicted_train, train_df[\"meanGrade\"], True)\n",
    "\n",
    "print(\"\\nDev performance:\")\n",
    "sse, mse = model_performance(predicted, dev_df[\"meanGrade\"], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test RMSE\n",
    "predicted_test = rfc.predict(test_idxs_padded.cpu().numpy())\n",
    "\n",
    "print(len(predicted_test))\n",
    "\n",
    "# evaluation\n",
    "print(\"\\nTest performance:\")\n",
    "sse, mse = model_performance(predicted_test, test_df[\"meanGrade\"], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
