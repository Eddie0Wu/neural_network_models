{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN application in 3D medical imaging\n",
    "\n",
    "This notebook contains the codes for applying CNN to medical imaging, e.g. predict a patient's age from brain MRI. It compares the direct CNN approach to the two stage (segmentation + supervised learning) approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install SimpleITK==1.2.4 \n",
    "\n",
    "! wget https://www.doc.ic.ac.uk/~bglocker/teaching/notebooks/brainage-data.zip\n",
    "! unzip brainage-data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = 'data/brain_age/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the meta data using pandas\n",
    "import pandas as pd\n",
    "\n",
    "meta_data_all = pd.read_csv(data_dir + 'meta/meta_data_all.csv')\n",
    "meta_data_all.head() # show the first five data entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "meta_data = meta_data_all\n",
    "\n",
    "sns.catplot(x=\"gender_text\", data=meta_data, kind=\"count\")\n",
    "plt.title('Gender distribution')\n",
    "plt.xlabel('Gender')\n",
    "plt.show()\n",
    "\n",
    "sns.distplot(meta_data['age'], bins=[10,20,30,40,50,60,70,80,90])\n",
    "plt.title('Age distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(range(len(meta_data['age'])),meta_data['age'], marker='.')\n",
    "plt.grid()\n",
    "plt.xlabel('Subject')\n",
    "plt.ylabel('Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate parameters low and high from window and level\n",
    "def wl_to_lh(window, level):\n",
    "    low = level - window/2\n",
    "    high = level + window/2\n",
    "    return low,high\n",
    "\n",
    "def display_image(img, x=None, y=None, z=None, window=None, level=None, colormap='gray', crosshair=False):\n",
    "    # Convert SimpleITK image to NumPy array\n",
    "    img_array = sitk.GetArrayFromImage(img)\n",
    "    \n",
    "    # Get image dimensions in millimetres\n",
    "    size = img.GetSize()\n",
    "    spacing = img.GetSpacing()\n",
    "    width  = size[0] * spacing[0]\n",
    "    height = size[1] * spacing[1]\n",
    "    depth  = size[2] * spacing[2]\n",
    "    \n",
    "    if x is None:\n",
    "        x = np.floor(size[0]/2).astype(int)\n",
    "    if y is None:\n",
    "        y = np.floor(size[1]/2).astype(int)\n",
    "    if z is None:\n",
    "        z = np.floor(size[2]/2).astype(int)\n",
    "    \n",
    "    if window is None:\n",
    "        window = np.max(img_array) - np.min(img_array)\n",
    "    \n",
    "    if level is None:\n",
    "        level = window / 2 + np.min(img_array)\n",
    "    \n",
    "    low,high = wl_to_lh(window,level)\n",
    "\n",
    "    # Display the orthogonal slices\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 4))\n",
    "\n",
    "    ax1.imshow(img_array[z,:,:], cmap=colormap, clim=(low, high), extent=(0, width, height, 0))\n",
    "    ax2.imshow(img_array[:,y,:], origin='lower', cmap=colormap, clim=(low, high), extent=(0, width,  0, depth))\n",
    "    ax3.imshow(img_array[:,:,x], origin='lower', cmap=colormap, clim=(low, high), extent=(0, height, 0, depth))\n",
    "\n",
    "    # Additionally display crosshairs\n",
    "    if crosshair:\n",
    "        ax1.axhline(y * spacing[1], lw=1)\n",
    "        ax1.axvline(x * spacing[0], lw=1)\n",
    "        ax2.axhline(z * spacing[2], lw=1)\n",
    "        ax2.axvline(x * spacing[0], lw=1)\n",
    "        ax3.axhline(z * spacing[2], lw=1)\n",
    "        ax3.axvline(y * spacing[1], lw=1)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def interactive_view(img):\n",
    "    size = img.GetSize() \n",
    "    img_array = sitk.GetArrayFromImage(img)\n",
    "    interact(display_image,img=fixed(img),\n",
    "             x=(0, size[0] - 1),\n",
    "             y=(0, size[1] - 1),\n",
    "             z=(0, size[2] - 1),\n",
    "             window=(0,np.max(img_array) - np.min(img_array)),\n",
    "             level=(np.min(img_array),np.max(img_array)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject with index 0\n",
    "ID = meta_data['subject_id'][0]\n",
    "age = meta_data['age'][0]\n",
    "\n",
    "# Image\n",
    "image_filename = data_dir + 'images/sub-' + ID + '_T1w_unbiased.nii.gz'\n",
    "img = sitk.ReadImage(image_filename)\n",
    "print('Image size and spance')\n",
    "print(img.GetSize())\n",
    "print(img.GetSpacing())\n",
    "\n",
    "# Mask\n",
    "mask_filename = data_dir + 'masks/sub-' + ID + '_T1w_brain_mask.nii.gz'\n",
    "msk = sitk.ReadImage(mask_filename)\n",
    "\n",
    "print('Imaging data of subject ' + ID + ' with age ' + str(age))\n",
    "\n",
    "print('\\nMR Image')\n",
    "display_image(img, window=400, level=200)\n",
    "\n",
    "print('Brain mask')\n",
    "display_image(msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up for brain tissue segmentation\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_mean_unit_var(image, mask):\n",
    "    \"\"\"Normalizes an image to zero mean and unit variance.\"\"\"\n",
    "\n",
    "    img_array = sitk.GetArrayFromImage(image)\n",
    "    img_array = img_array.astype(np.float32)\n",
    "\n",
    "    msk_array = sitk.GetArrayFromImage(mask)\n",
    "\n",
    "    mean = np.mean(img_array[msk_array>0])\n",
    "    std = np.std(img_array[msk_array>0])\n",
    "\n",
    "    if std > 0:\n",
    "        img_array = (img_array - mean) / std\n",
    "        img_array[msk_array==0] = 0\n",
    "\n",
    "    image_normalised = sitk.GetImageFromArray(img_array)\n",
    "    image_normalised.CopyInformation(image)\n",
    "\n",
    "    return image_normalised\n",
    "\n",
    "\n",
    "def resample_image(image, out_spacing=(1.0, 1.0, 1.0), out_size=None, is_label=False, pad_value=0):\n",
    "    \"\"\"Resamples an image to given element spacing and output size.\"\"\"\n",
    "\n",
    "    original_spacing = np.array(image.GetSpacing())\n",
    "    original_size = np.array(image.GetSize())\n",
    "\n",
    "    if out_size is None:\n",
    "        out_size = np.round(np.array(original_size * original_spacing / np.array(out_spacing))).astype(int)\n",
    "    else:\n",
    "        out_size = np.array(out_size)\n",
    "\n",
    "    original_direction = np.array(image.GetDirection()).reshape(len(original_spacing),-1)\n",
    "    original_center = (np.array(original_size, dtype=float) - 1.0) / 2.0 * original_spacing\n",
    "    out_center = (np.array(out_size, dtype=float) - 1.0) / 2.0 * np.array(out_spacing)\n",
    "\n",
    "    original_center = np.matmul(original_direction, original_center)\n",
    "    out_center = np.matmul(original_direction, out_center)\n",
    "    out_origin = np.array(image.GetOrigin()) + (original_center - out_center)\n",
    "\n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetOutputSpacing(out_spacing)\n",
    "    resample.SetSize(out_size.tolist())\n",
    "    resample.SetOutputDirection(image.GetDirection())\n",
    "    resample.SetOutputOrigin(out_origin.tolist())\n",
    "    resample.SetTransform(sitk.Transform())\n",
    "    resample.SetDefaultPixelValue(pad_value)\n",
    "\n",
    "    if is_label:\n",
    "        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n",
    "    else:\n",
    "        resample.SetInterpolator(sitk.sitkBSpline)\n",
    "\n",
    "    return resample.Execute(image)\n",
    "\n",
    "\n",
    "class ImageSegmentationDataset(Dataset):\n",
    "    \"\"\"Dataset for image segmentation.\"\"\"\n",
    "\n",
    "    def __init__(self, file_list_img, file_list_seg, file_list_msk, img_spacing, img_size):\n",
    "        self.samples = []\n",
    "        self.img_names = []\n",
    "        self.seg_names = []\n",
    "        for idx, _ in enumerate(tqdm(range(len(file_list_img)), desc='Loading Data')):\n",
    "            img_path = file_list_img[idx]\n",
    "            seg_path = file_list_seg[idx]\n",
    "            msk_path = file_list_msk[idx]\n",
    "\n",
    "            img = sitk.ReadImage(img_path, sitk.sitkFloat32)\n",
    "\n",
    "            seg = sitk.ReadImage(seg_path, sitk.sitkInt64)\n",
    "\n",
    "            msk = sitk.ReadImage(msk_path, sitk.sitkUInt8)\n",
    "\n",
    "            #pre=processing\n",
    "            img = zero_mean_unit_var(img, msk)\n",
    "            img = resample_image(img, img_spacing, img_size, is_label=False)\n",
    "            seg = resample_image(seg, img_spacing, img_size, is_label=True)\n",
    "            msk = resample_image(msk, img_spacing, img_size, is_label=True)\n",
    "\n",
    "            sample = {'img': img, 'seg': seg, 'msk': msk}\n",
    "\n",
    "            self.samples.append(sample)\n",
    "            self.img_names.append(os.path.basename(img_path))\n",
    "            self.seg_names.append(os.path.basename(seg_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        sample = self.samples[item]\n",
    "\n",
    "        image = torch.from_numpy(sitk.GetArrayFromImage(sample['img'])).unsqueeze(0)\n",
    "        seg = torch.from_numpy(sitk.GetArrayFromImage(sample['seg'])).unsqueeze(0)\n",
    "        msk = torch.from_numpy(sitk.GetArrayFromImage(sample['msk'])).unsqueeze(0)\n",
    "\n",
    "        return {'img': image, 'seg': seg, 'msk': msk}\n",
    "\n",
    "    def get_sample(self, item):\n",
    "        return self.samples[item]\n",
    "\n",
    "    def get_img_name(self, item):\n",
    "        return self.img_names[item]\n",
    "\n",
    "    def get_seg_name(self, item):\n",
    "        return self.seg_names[item]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "cuda_dev = '0' #GPU device 0 (can be changed if multiple GPUs are available)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:\" + cuda_dev if use_cuda else \"cpu\")\n",
    "\n",
    "print('Device: ' + str(device))\n",
    "if use_cuda:\n",
    "    print('GPU: ' + str(torch.cuda.get_device_name(int(cuda_dev))))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_seed = 42 #fixed random seed\n",
    "\n",
    "img_size = [96, 96, 96]\n",
    "img_spacing = [2, 2, 2]\n",
    "\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 2\n",
    "val_interval = 10\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "out_dir = './output'\n",
    "\n",
    "# Create output directory\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "meta_data_seg_train = pd.read_csv(data_dir + 'meta/meta_data_segmentation_train.csv')\n",
    "ids_seg_train = list(meta_data_seg_train['subject_id'])\n",
    "files_seg_img_train = [data_dir + 'images/sub-' + f + '_T1w_unbiased.nii.gz' for f in ids_seg_train]\n",
    "files_seg_seg_train = [data_dir + 'segs_refs/sub-' + f + '_T1w_seg.nii.gz' for f in ids_seg_train]\n",
    "files_seg_msk_train = [data_dir + 'masks/sub-' + f + '_T1w_brain_mask.nii.gz' for f in ids_seg_train]\n",
    "\n",
    "meta_data_seg_val = pd.read_csv(data_dir + 'meta/meta_data_segmentation_val.csv')\n",
    "ids_seg_val = list(meta_data_seg_val['subject_id'])\n",
    "files_seg_img_val = [data_dir + 'images/sub-' + f + '_T1w_unbiased.nii.gz' for f in ids_seg_val]\n",
    "files_seg_seg_val = [data_dir + 'segs_refs/sub-' + f + '_T1w_seg.nii.gz' for f in ids_seg_val]\n",
    "files_seg_msk_val = [data_dir + 'masks/sub-' + f + '_T1w_brain_mask.nii.gz' for f in ids_seg_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Into DataLoaders\n",
    "\n",
    "# LOAD TRAINING DATA\n",
    "dataset_train = ImageSegmentationDataset(files_seg_img_train, files_seg_seg_train, files_seg_msk_train, img_spacing, img_size)\n",
    "# FOR QUICK DEBUGGING, USE THE VALIDATION DATA FOR TRAINING\n",
    "#dataset_train = ImageSegmentationDataset(files_seg_img_val, files_seg_seg_val, files_seg_msk_val, img_spacing, img_size)\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# LOAD VALIDATION DATA\n",
    "dataset_val = ImageSegmentationDataset(files_seg_img_val, files_seg_seg_val, files_seg_msk_val, img_spacing, img_size)\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise training samples\n",
    "sample = dataset_train.get_sample(0)\n",
    "img_name = dataset_train.get_img_name(0)\n",
    "seg_name = dataset_train.get_seg_name(0)\n",
    "print('sample size and spacing')\n",
    "print(sample['img'].GetSize())\n",
    "print(sample['img'].GetSpacing())\n",
    "print()\n",
    "\n",
    "a = sitk.LabelToRGB(sample['seg'])\n",
    "a = sitk.GetArrayFromImage(a)\n",
    "print(a[32][32][32])\n",
    "\n",
    "print('Image: ' + img_name)\n",
    "display_image(sample['img'], window=5, level=0)\n",
    "print('Segmentation')\n",
    "display_image(sitk.LabelToRGB(sample['seg']))\n",
    "print('Mask')\n",
    "display_image(sample['msk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "\n",
    "# Input x: [batch_size, channel, 64, 64, 64]\n",
    "\n",
    "# pool of square window of size=2, stride=2\n",
    "# m = nn.MaxPool3d(2, stride=2)\n",
    "# input = torch.randn(20, 16, 50, 44, 31)\n",
    "# output = m(input)\n",
    "# print(output.shape)\n",
    "\n",
    "# With square kernels and equal stride\n",
    "# m = nn.ConvTranspose3d(16, 33, 2, stride=2)\n",
    "# m = nn.ConvTranspose3d(16, 33, 3, stride=1)\n",
    "# input = torch.randn(20, 16, 1, 2, 4)\n",
    "# output = m(input)\n",
    "# print(output.shape)\n",
    "\n",
    "class conv(nn.Module):\n",
    "    def __init__(self, input, output):\n",
    "        super(conv, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv3d(input, output, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(output),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(output, output, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(output),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class down_samp(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super(down_samp, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv3d(c, c, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(c),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class up_samp(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super(up_samp, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.ConvTranspose3d(c, c // 2, kernel_size=2, stride=2, bias=False),\n",
    "            nn.BatchNorm3d(c // 2),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class SimpleNet3D(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleNet3D, self).__init__()\n",
    "        # self.conv1 = nn.Conv3d(1, 4, kernel_size=3, padding=1)\n",
    "        # self.conv2 = nn.Conv3d(4, num_classes, kernel_size=3, padding=1)\n",
    "\n",
    "\n",
    "\n",
    "        self.down_conv_1 = conv(1, 64)\n",
    "        self.down_samp_1 = down_samp(64)\n",
    "        self.down_conv_2 = conv(64, 128)\n",
    "        self.down_samp_2 = down_samp(128)\n",
    "        self.down_conv_3 = conv(128, 256)\n",
    "        self.down_samp_3 = down_samp(256)\n",
    "        self.down_conv_4 = conv(256, 512)\n",
    "        self.down_samp_4 = down_samp(512)\n",
    "\n",
    "        self.bottom = conv(512, 1024)\n",
    "\n",
    "        self.up_samp_1 = up_samp(1024)\n",
    "        self.up_conv_1 = conv(1024, 512)\n",
    "        self.up_samp_2 = up_samp(512)\n",
    "        self.up_conv_2 = conv(512, 256)\n",
    "        self.up_samp_3 = up_samp(256)\n",
    "        self.up_conv_3 = conv(256, 128)\n",
    "        self.up_samp_4 = up_samp(128)\n",
    "        self.up_conv_4 = conv(128, 64)\n",
    "\n",
    "        self.out = nn.Conv3d(64, num_classes, kernel_size=1, stride=1, bias=False)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        d1c = self.down_conv_1(x)\n",
    "        d1s = self.down_samp_1(d1c)\n",
    "        d2c = self.down_conv_2(d1s)\n",
    "        d2s = self.down_samp_2(d2c)\n",
    "        d3c = self.down_conv_3(d2s)\n",
    "        d3s = self.down_samp_3(d3c)\n",
    "        d4c = self.down_conv_4(d3s)\n",
    "        d4s = self.down_samp_4(d4c)\n",
    "\n",
    "        x_bottom = self.bottom(d4s)\n",
    "\n",
    "        u1 = self.up_samp_1(x_bottom)\n",
    "        u1 = torch.cat((d4c, u1), dim=1)\n",
    "        u1 = self.up_conv_1(u1)\n",
    "        u2 = self.up_samp_2(u1)\n",
    "        u2 = torch.cat((d3c, u2), dim=1)\n",
    "        u2 = self.up_conv_2(u2)\n",
    "        u3 = self.up_samp_3(u2)\n",
    "        u3 = torch.cat((d2c, u3), dim=1)\n",
    "        u3 = self.up_conv_3(u3)\n",
    "        u4 = self.up_samp_4(u3)\n",
    "        u4 = torch.cat((d1c, u4), dim=1)\n",
    "        u4 = self.up_conv_4(u4)\n",
    "\n",
    "        x = self.out(u4)\n",
    "        \n",
    "        \n",
    "        return x # cross-entropy loss expects raw logits and applies softmax\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# test = SimpleNet3D(num_classes=num_classes).to('cpu')\n",
    "# for batch_idx, batch_samples in enumerate(dataloader_train):\n",
    "#   x = batch_samples['img'].to('cpu')\n",
    "#   break\n",
    "# with torch.no_grad():\n",
    "#   p = test(x)\n",
    "# del p\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "\n",
    "model_dir = os.path.join(out_dir, 'model')\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "torch.manual_seed(rnd_seed) #fix random seed\n",
    "\n",
    "model = SimpleNet3D(num_classes=num_classes).to(device)\n",
    "model.train()\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_train_log = []\n",
    "loss_val_log = []\n",
    "epoch_val_log = []\n",
    "    \n",
    "print('START TRAINING...')\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "    # Training\n",
    "    for batch_idx, batch_samples in enumerate(dataloader_train):\n",
    "        img, seg = batch_samples['img'].to(device), batch_samples['seg'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        prd = model(img)\n",
    "        loss = F.cross_entropy(prd, seg.squeeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss_train_log.append(loss.item())\n",
    "\n",
    "    print('+ TRAINING \\tEpoch: {} \\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
    "    \n",
    "    # Validation\n",
    "    if epoch == 1 or epoch % val_interval == 0:\n",
    "        loss_val = 0\n",
    "        sum_pts = 0\n",
    "        with torch.no_grad():\n",
    "            for data_sample in dataloader_val:\n",
    "                img, seg = data_sample['img'].to(device), data_sample['seg'].to(device)\n",
    "                prd = model(img)\n",
    "                loss_val += F.cross_entropy(prd, seg.squeeze(1), reduction='sum').item()\n",
    "                sum_pts += np.prod(img_size)\n",
    "                \n",
    "        prd = torch.argmax(prd, dim=1)\n",
    "        prediction = sitk.GetImageFromArray(prd.cpu().squeeze().numpy().astype(np.uint8))\n",
    "        \n",
    "\n",
    "        loss_val /= sum_pts\n",
    "\n",
    "        loss_val_log.append(loss_val)\n",
    "        epoch_val_log.append(epoch)\n",
    "\n",
    "        print('--------------------------------------------------')\n",
    "        print('+ VALIDATE \\tEpoch: {} \\tLoss: {:.6f}'.format(epoch, loss_val))\n",
    "        display_image(sitk.LabelToRGB(prediction))\n",
    "        print('--------------------------------------------------')\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(model_dir, 'model.pt'))\n",
    "\n",
    "print('\\nFinished TRAINING.')\n",
    "\n",
    "plt.plot(range(1, num_epochs + 1), loss_train_log, c='r', label='train')\n",
    "plt.plot(epoch_val_log, loss_val_log, c='b', label='val')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess test data\n",
    "meta_data_reg_train = pd.read_csv(data_dir + 'meta/meta_data_regression_train.csv')\n",
    "ids_seg_test = list(meta_data_reg_train['subject_id'])\n",
    "files_seg_img_test = [data_dir + 'images/sub-' + f + '_T1w_unbiased.nii.gz' for f in ids_seg_test]\n",
    "files_seg_seg_test = [data_dir + 'segs_refs/sub-' + f + '_T1w_seg.nii.gz' for f in ids_seg_test]\n",
    "files_seg_msk_test = [data_dir + 'masks/sub-' + f + '_T1w_brain_mask.nii.gz' for f in ids_seg_test]\n",
    "\n",
    "dataset_test = ImageSegmentationDataset(files_seg_img_test, files_seg_seg_test, files_seg_msk_test, img_spacing, img_size)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise testing samples\n",
    "sample = dataset_test.get_sample(0)\n",
    "img_name = dataset_test.get_img_name(0)\n",
    "seg_name = dataset_test.get_seg_name(0)\n",
    "print('Image: ' + img_name)\n",
    "display_image(sample['img'], window=5, level=0)\n",
    "print('Segmentation')\n",
    "display_image(sitk.LabelToRGB(sample['seg']))\n",
    "\n",
    "print('Mask')\n",
    "display_image(sample['msk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing \n",
    "pred_dir = os.path.join(out_dir, 'pred')\n",
    "if not os.path.exists(pred_dir):\n",
    "    os.makedirs(pred_dir)\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "model = SimpleNet3D(num_classes=num_classes)\n",
    "model_dir = 'drive/MyDrive/Colab Notebooks/ML for Img/'\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, 'model.pt')))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "    \n",
    "print('START TESTING...')\n",
    "\n",
    "\n",
    "precision0 = np.zeros(500)\n",
    "recall0 = np.zeros(500)\n",
    "precision1 = np.zeros(500)\n",
    "recall1 = np.zeros(500)\n",
    "precision2 = np.zeros(500)\n",
    "recall2 = np.zeros(500)\n",
    "precision3 = np.zeros(500)\n",
    "recall3 = np.zeros(500)\n",
    "acc = np.zeros(500)\n",
    "f10 = np.zeros(500)\n",
    "f11 = np.zeros(500)\n",
    "f12 = np.zeros(500)\n",
    "f13 = np.zeros(500)\n",
    "\n",
    "\n",
    "loss_test = 0\n",
    "sum_pts = 0\n",
    "idx_test = 0\n",
    "with torch.no_grad():\n",
    "    for data_sample in dataloader_test:\n",
    "        img, seg = data_sample['img'].to(device), data_sample['seg'].to(device)\n",
    "        prd = model(img)\n",
    "        loss_test += F.cross_entropy(prd, seg.squeeze(1), reduction='sum').item()\n",
    "        sum_pts += np.prod(img_size)\n",
    "        \n",
    "        prd = torch.argmax(prd, dim=1)\n",
    "\n",
    "        # get pred and truth seg\n",
    "        t = seg.view(-1, 1).cpu().numpy()\n",
    "        p = prd.view(-1, 1).cpu().numpy()\n",
    "\n",
    "        acc[idx_test] = metrics.accuracy_score(t, p)\n",
    "\n",
    "        # find label 0 in prediction, print(np.sum(p == 0))\n",
    "        macro_precision = metrics.precision_score(t, p, average=None)\n",
    "        macro_recall = metrics.recall_score(t, p, average=None)\n",
    "        precision0[idx_test] = macro_precision[0]\n",
    "        precision1[idx_test] = macro_precision[1]\n",
    "        precision2[idx_test] = macro_precision[2]\n",
    "        precision3[idx_test] = macro_precision[3]\n",
    "        recall0[idx_test] = macro_recall[0]\n",
    "        recall1[idx_test] = macro_recall[1]\n",
    "        recall2[idx_test] = macro_recall[2]\n",
    "        recall3[idx_test] = macro_recall[3]\n",
    "\n",
    "        f10[idx_test] = 2 * macro_precision[0] * macro_recall[0] / (macro_precision[0] + macro_recall[0])\n",
    "        f11[idx_test] = 2 * macro_precision[1] * macro_recall[1] / (macro_precision[1] + macro_recall[1])\n",
    "        f12[idx_test] = 2 * macro_precision[2] * macro_recall[2] / (macro_precision[2] + macro_recall[2])\n",
    "        f13[idx_test] = 2 * macro_precision[3] * macro_recall[3] / (macro_precision[3] + macro_recall[3])\n",
    "\n",
    "        sample = dataset_test.get_sample(idx_test)\n",
    "        name = dataset_test.get_seg_name(idx_test)\n",
    "        prediction = sitk.GetImageFromArray(prd.cpu().squeeze().numpy().astype(np.uint8))\n",
    "        prediction.CopyInformation(sample['seg'])\n",
    "        sitk.WriteImage(prediction, os.path.join(pred_dir, name))\n",
    "        \n",
    "        idx_test += 1\n",
    "        if idx_test % 50 == 0: print('\\t tested ' + str(idx_test))\n",
    "\n",
    "# show acc\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Acc Plot')\n",
    "ax1.boxplot(acc, sym = 'o', vert = True, whis=1.5, patch_artist = True, meanline = False, showmeans = True, showbox = True, showfliers = True)\n",
    "plt.show()\n",
    "# show precision\n",
    "data = np.concatenate((precision0.reshape((500, 1)), precision1.reshape((500, 1)), precision2.reshape((500, 1)), precision3.reshape((500, 1))), axis=1)\n",
    "data = pd.DataFrame(data, columns=['Background','CSF','GM','WM'])\n",
    "data.boxplot(sym = 'o', vert = True, whis=1.5, patch_artist = True, meanline = False, showmeans = True, showbox = True, showfliers = True)\n",
    "plt.title('Precision Plot')\n",
    "plt.show()\n",
    "# show recall\n",
    "data = np.concatenate((recall0.reshape((500, 1)), recall1.reshape((500, 1)), recall2.reshape((500, 1)), recall3.reshape((500, 1))), axis=1)\n",
    "data = pd.DataFrame(data, columns=['Background','CSF','GM','WM'])\n",
    "data.boxplot(sym = 'o', vert = True, whis=1.5, patch_artist = True, meanline = False, showmeans = True, showbox = True, showfliers = True)\n",
    "plt.title('Recall Plot')\n",
    "plt.show()\n",
    "# show Dice/F1\n",
    "data = np.concatenate((f10.reshape((500, 1)), f11.reshape((500, 1)), f12.reshape((500, 1)), f13.reshape((500, 1))), axis=1)\n",
    "data = pd.DataFrame(data, columns=['Background','CSF','GM','WM'])\n",
    "data.boxplot(sym = 'o', vert = True, whis=1.5, patch_artist = True, meanline = False, showmeans = True, showbox = True, showfliers = True)\n",
    "plt.title('Dice/F1 Plot')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# macro_precision = np.mean(precision0.mean() + precision1.mean() + precision2.mean() + precision3.mean()) / 500\n",
    "# macro_recall = np.mean(recall0.mean() + recall1.mean() + recall2.mean() + recall3.mean()) / 500\n",
    "# print('+ TESTING \\tmacro precision: {:.6f}'.format(macro_precision))\n",
    "# print('+ TESTING \\tmacro recall: {:.6f}'.format(macro_recall))\n",
    "# macro_f1 = 2 * macro_precision * macro_recall / (macro_precision + macro_recall)\n",
    "# print('+ TESTING \\tDice/F1 score: {:.6f}'.format(macro_f1))\n",
    "        \n",
    "loss_test /= sum_pts\n",
    "\n",
    "print('+ TESTING \\tLoss: {:.6f}'.format(loss_test))\n",
    "\n",
    "# Show last testing sample as an example\n",
    "print('\\n\\nReference segmentation')\n",
    "display_image(sitk.LabelToRGB(sample['seg']))\n",
    "print('Predicted segmentation')\n",
    "display_image(sitk.LabelToRGB(prediction))\n",
    "\n",
    "print('\\nFinished TESTING.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating absolute tissue volume\n",
    "\n",
    "import os\n",
    "\n",
    "# USE THIS TO RUN THE CALCULATIONS ON YOUR SEGMENTATONS\n",
    "seg_dir = './output/pred/'\n",
    "\n",
    "# USE THIS TO RUN THE CALCULATIONS ON OUR REFERENCE SEGMENTATIONS\n",
    "# seg_dir = data_dir + 'segs_refs/'\n",
    "\n",
    "meta_data_reg_train = pd.read_csv(data_dir + 'meta/meta_data_regression_train.csv')\n",
    "\n",
    "ids_reg_train = list(meta_data_reg_train['subject_id'])\n",
    "files_reg_seg_train = [seg_dir + 'sub-' + f + '_T1w_seg.nii.gz' for f in ids_reg_train]\n",
    "\n",
    "# THIS MATRIX WILL STORE THE VOLUMES PER TISSUE CLASS\n",
    "vols = np.zeros((3,len(files_reg_seg_train)))\n",
    "\n",
    "for idx, _ in enumerate(tqdm(range(len(files_reg_seg_train)), desc='Calculating Features')):\n",
    "    \n",
    "    seg_filename = files_reg_seg_train[idx]\n",
    "    \n",
    "    if os.path.exists(seg_filename):\n",
    "        seg = sitk.ReadImage(seg_filename)\n",
    "        \n",
    "        ########################################\n",
    "        # ADD YOUR CODE HERE\n",
    "        ########################################\n",
    "        seg_array = sitk.GetArrayFromImage(seg)\n",
    "        for i in range(3):\n",
    "            vols[i,idx] = np.count_nonzero(seg_array == i+1)\n",
    "            \n",
    "print(vols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.scatter(vols[0,:],meta_data_reg_train['age'], marker='.')\n",
    "plt.scatter(vols[1,:],meta_data_reg_train['age'], marker='.')\n",
    "plt.scatter(vols[2,:],meta_data_reg_train['age'], marker='.')\n",
    "plt.grid()\n",
    "plt.title('Unnormalised')\n",
    "plt.xlabel('Volume')\n",
    "plt.ylabel('Age')\n",
    "plt.legend(('CSF','GM','WM'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating relative tissue volume\n",
    "\n",
    "vols_normalised = np.zeros((3,len(files_reg_seg_train)))\n",
    "\n",
    "########################################\n",
    "# ADD YOUR CODE HERE\n",
    "########################################\n",
    "\n",
    "total = np.sum(vols, axis = 0)\n",
    "vols_normalised = vols / total\n",
    "print(vols_normalised.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.scatter(vols_normalised[0,:],meta_data_reg_train['age'], marker='.')\n",
    "plt.scatter(vols_normalised[1,:],meta_data_reg_train['age'], marker='.')\n",
    "plt.scatter(vols_normalised[2,:],meta_data_reg_train['age'], marker='.')\n",
    "plt.grid()\n",
    "plt.title('Normalised')\n",
    "plt.xlabel('Volume')\n",
    "plt.ylabel('Age')\n",
    "plt.legend(('CSF','GM','WM'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for age regression\n",
    "X = vols_normalised.T\n",
    "y = meta_data_reg_train['age'].values.reshape(-1,1)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run regression\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# normal y\n",
    "min = 10\n",
    "max = 100\n",
    "y_normal = (y - min) / (max - min)\n",
    "y_normal = y_normal.reshape(y_normal.shape[0])\n",
    "\n",
    "best_nn = None\n",
    "min_MAE = 1024\n",
    "learning_rate = [0.0001, 0.0005, 0.001, 0.002, 0.005]\n",
    "alpha = [0.00001, 0.0001, 0.001]\n",
    "score = np.zeros((3, 5))\n",
    "i, j = 0, 0\n",
    "for a in alpha:\n",
    "  j = 0\n",
    "  for lr in learning_rate:\n",
    "    print('LR = '+str(lr)+',\\t alpha = '+str(a))\n",
    "    MAE = 0\n",
    "\n",
    "    kfold = KFold(n_splits = 2, shuffle = False, random_state = None)\n",
    "    for idx_train, idx_validation in kfold.split(X):\n",
    "      MLP = MLPRegressor(\n",
    "        hidden_layer_sizes=(1024, 256, 64, 16),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=a, # L2 parameter\n",
    "        batch_size=25,\n",
    "        learning_rate='invscaling',\n",
    "        learning_rate_init=lr,\n",
    "        max_iter=200,\n",
    "        shuffle=True\n",
    "      )\n",
    "\n",
    "      X_train, X_vali = X[idx_train], X[idx_validation]\n",
    "      y_train, y_vali = y_normal[idx_train], y_normal[idx_validation]\n",
    "      \n",
    "\n",
    "      MLP.fit(X_train, y_train)\n",
    "      prediction = MLP.predict(X_vali)\n",
    "      MAE += mean_absolute_error(y_vali, prediction)\n",
    "\n",
    "    score[i][j] += MAE / 2.\n",
    "\n",
    "    if score[i][j] < min_MAE:\n",
    "      min_MAE = score[i][j]\n",
    "      best_nn = MLP\n",
    "\n",
    "    j += 1\n",
    "\n",
    "  i += 1\n",
    "\n",
    "print('min MAE = ', min_MAE)\n",
    "idx = np.array([score == np.min(score)]).reshape((3, 5))\n",
    "for i in range(len(alpha)):\n",
    "  for j in range(len(learning_rate)):\n",
    "    if idx[i][j] == True:\n",
    "      print('alpha, lr, MAE = ', alpha[i], learning_rate[j], score[i][j], 'Min')\n",
    "    else:\n",
    "      print('alpha, lr, MAE = ', alpha[i], learning_rate[j], score[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "predicted = best_nn.predict(X)\n",
    "predicted = predicted * (max - min) + min\n",
    "\n",
    "print('MAE: {0}'.format(mean_absolute_error(y,predicted)))\n",
    "print('R2: {0}'.format(r2_score(y,predicted)))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y, predicted, marker='.')\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
    "ax.set_xlabel('Real Age')\n",
    "ax.set_ylabel('Predicted Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other regression attempts\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn import svm, tree\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "svm_reg = svm.SVR()\n",
    "tree_reg = tree.DecisionTreeRegressor()\n",
    "rf_reg = RandomForestRegressor(max_depth = 4, random_state = rnd_seed)\n",
    "ada_reg = AdaBoostRegressor(n_estimators = 50, learning_rate = 0.3)\n",
    "grad_reg = GradientBoostingRegressor()\n",
    "ridge_reg = Ridge(alpha = 0.01)\n",
    "\n",
    "\n",
    "# Ridge Regression\n",
    "scoring = [\"neg_mean_absolute_error\", \"r2\"]\n",
    "scores = cross_validate(ridge_reg, X, y, scoring = scoring, cv = 2)\n",
    "keys = sorted(scores.keys())\n",
    "print(scores[keys[2]])\n",
    "print(scores[keys[3]])\n",
    "\n",
    "ridge_reg.fit(X, y)\n",
    "predicted = ridge_reg.predict(X)\n",
    "\n",
    "# Ridge regression plot\n",
    "\n",
    "print('MAE: {0}'.format(mean_absolute_error(y,predicted)))\n",
    "print('R2: {0}'.format(r2_score(y,predicted)))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y, predicted, marker='.')\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
    "ax.set_xlabel('Real Age')\n",
    "ax.set_ylabel('Predicted Age')\n",
    "plt.show()\n",
    "fig.savefig(\"ridge_reg_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support vector machine\n",
    "\n",
    "scoring = [\"neg_mean_absolute_error\", \"r2\"]\n",
    "scores = cross_validate(svm_reg, X, y.reshape(y.shape[0],), scoring = scoring, cv = 2)\n",
    "keys = sorted(scores.keys())\n",
    "print(scores[keys[2]])\n",
    "print(scores[keys[3]])\n",
    "\n",
    "svm_reg.fit(X, y)\n",
    "predicted = svm_reg.predict(X)\n",
    "\n",
    "# Support vector machine plot\n",
    "\n",
    "print('MAE: {0}'.format(mean_absolute_error(y,predicted)))\n",
    "print('R2: {0}'.format(r2_score(y,predicted)))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y, predicted, marker='.')\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
    "ax.set_xlabel('Real Age')\n",
    "ax.set_ylabel('Predicted Age')\n",
    "plt.show()\n",
    "fig.savefig(\"ridge_reg_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest regression\n",
    "\n",
    "scoring = [\"neg_mean_absolute_error\", \"r2\"]\n",
    "scores = cross_validate(rf_reg, X, y.reshape(y.shape[0],), scoring = scoring, cv = 2)\n",
    "keys = sorted(scores.keys())\n",
    "print(scores[keys[2]])\n",
    "print(scores[keys[3]])\n",
    "\n",
    "rf_reg.fit(X, y)\n",
    "predicted = rf_reg.predict(X)\n",
    "\n",
    "# Random forest regression plot\n",
    "\n",
    "print('MAE: {0}'.format(mean_absolute_error(y,predicted)))\n",
    "print('R2: {0}'.format(r2_score(y,predicted)))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y, predicted, marker='.')\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
    "ax.set_xlabel('Real Age')\n",
    "ax.set_ylabel('Predicted Age')\n",
    "plt.show()\n",
    "fig.savefig(\"randomforest_reg_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost regression\n",
    "\n",
    "scoring = [\"neg_mean_absolute_error\", \"r2\"]\n",
    "scores = cross_validate(ada_reg, X, y.reshape(y.shape[0],), scoring = scoring, cv = 2)\n",
    "keys = sorted(scores.keys())\n",
    "print(scores[keys[2]])\n",
    "print(scores[keys[3]])\n",
    "\n",
    "ada_reg.fit(X, y)\n",
    "predicted = ada_reg.predict(X)\n",
    "\n",
    "# Adaboost regression plot\n",
    "\n",
    "print('MAE: {0}'.format(mean_absolute_error(y,predicted)))\n",
    "print('R2: {0}'.format(r2_score(y,predicted)))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y, predicted, marker='.')\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
    "ax.set_xlabel('Real Age')\n",
    "ax.set_ylabel('Predicted Age')\n",
    "plt.show()\n",
    "fig.savefig(\"ada_reg_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on held-out data\n",
    "! wget https://www.doc.ic.ac.uk/~bglocker/teaching/notebooks/brainage-testdata.zip\n",
    "! unzip brainage-testdata.zip\n",
    "\n",
    "# Loading data\n",
    "meta_data_reg_test = pd.read_csv(data_dir + 'meta/meta_data_regression_test.csv')\n",
    "ids_seg_test = list(meta_data_reg_test['subject_id'])\n",
    "files_seg_img_test = [data_dir + 'images/sub-' + f + '_T1w_unbiased.nii.gz' for f in ids_seg_test]\n",
    "files_seg_seg_test = [data_dir + 'segs_refs/sub-' + f + '_T1w_seg.nii.gz' for f in ids_seg_test]\n",
    "files_seg_msk_test = [data_dir + 'masks/sub-' + f + '_T1w_brain_mask.nii.gz' for f in ids_seg_test]\n",
    "\n",
    "dataset_test = ImageSegmentationDataset(files_seg_img_test, files_seg_seg_test, files_seg_msk_test, img_spacing, img_size)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run test\n",
    "pred_dir = os.path.join(out_dir, 'final_test')\n",
    "if not os.path.exists(pred_dir):\n",
    "    os.makedirs(pred_dir)\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "model = SimpleNet3D(num_classes=num_classes)\n",
    "model_dir = 'drive/MyDrive/Colab Notebooks/ML for Img/'\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, 'model.pt')))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "    \n",
    "print('START TESTING...')\n",
    "\n",
    "num = 100\n",
    "precision0 = np.zeros(num)\n",
    "recall0 = np.zeros(num)\n",
    "precision1 = np.zeros(num)\n",
    "recall1 = np.zeros(num)\n",
    "precision2 = np.zeros(num)\n",
    "recall2 = np.zeros(num)\n",
    "precision3 = np.zeros(num)\n",
    "recall3 = np.zeros(num)\n",
    "acc = np.zeros(num)\n",
    "f10 = np.zeros(num)\n",
    "f11 = np.zeros(num)\n",
    "f12 = np.zeros(num)\n",
    "f13 = np.zeros(num)\n",
    "\n",
    "\n",
    "loss_test = 0\n",
    "sum_pts = 0\n",
    "idx_test = 0\n",
    "with torch.no_grad():\n",
    "    for data_sample in dataloader_test:\n",
    "        img, seg = data_sample['img'].to(device), data_sample['seg'].to(device)\n",
    "        prd = model(img)\n",
    "        loss_test += F.cross_entropy(prd, seg.squeeze(1), reduction='sum').item()\n",
    "        sum_pts += np.prod(img_size)\n",
    "        \n",
    "        prd = torch.argmax(prd, dim=1)\n",
    "\n",
    "        # get pred and truth seg\n",
    "        t = seg.view(-1, 1).cpu().numpy()\n",
    "        p = prd.view(-1, 1).cpu().numpy()\n",
    "\n",
    "        acc[idx_test] = metrics.accuracy_score(t, p)\n",
    "\n",
    "        # find label 0 in prediction, print(np.sum(p == 0))\n",
    "        macro_precision = metrics.precision_score(t, p, average=None)\n",
    "        macro_recall = metrics.recall_score(t, p, average=None)\n",
    "        precision0[idx_test] = macro_precision[0]\n",
    "        precision1[idx_test] = macro_precision[1]\n",
    "        precision2[idx_test] = macro_precision[2]\n",
    "        precision3[idx_test] = macro_precision[3]\n",
    "        recall0[idx_test] = macro_recall[0]\n",
    "        recall1[idx_test] = macro_recall[1]\n",
    "        recall2[idx_test] = macro_recall[2]\n",
    "        recall3[idx_test] = macro_recall[3]\n",
    "\n",
    "        f10[idx_test] = 2 * macro_precision[0] * macro_recall[0] / (macro_precision[0] + macro_recall[0])\n",
    "        f11[idx_test] = 2 * macro_precision[1] * macro_recall[1] / (macro_precision[1] + macro_recall[1])\n",
    "        f12[idx_test] = 2 * macro_precision[2] * macro_recall[2] / (macro_precision[2] + macro_recall[2])\n",
    "        f13[idx_test] = 2 * macro_precision[3] * macro_recall[3] / (macro_precision[3] + macro_recall[3])\n",
    "\n",
    "        sample = dataset_test.get_sample(idx_test)\n",
    "        name = dataset_test.get_seg_name(idx_test)\n",
    "        prediction = sitk.GetImageFromArray(prd.cpu().squeeze().numpy().astype(np.uint8))\n",
    "        prediction.CopyInformation(sample['seg'])\n",
    "        sitk.WriteImage(prediction, os.path.join(pred_dir, name))\n",
    "        \n",
    "        idx_test += 1\n",
    "        if idx_test % 10 == 0: print('\\t tested ' + str(idx_test))\n",
    "\n",
    "# show acc\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Acc Plot')\n",
    "ax1.boxplot(acc, sym = 'o', vert = True, whis=1.5, patch_artist = True, meanline = False, showmeans = True, showbox = True, showfliers = True)\n",
    "plt.show()\n",
    "# show precision\n",
    "data = np.concatenate((precision0.reshape((num, 1)), precision1.reshape((num, 1)), precision2.reshape((num, 1)), precision3.reshape((num, 1))), axis=1)\n",
    "data = pd.DataFrame(data, columns=['Background','CSF','GM','WM'])\n",
    "data.boxplot(sym = 'o', vert = True, whis=1.5, patch_artist = True, meanline = False, showmeans = True, showbox = True, showfliers = True)\n",
    "plt.title('Precision Plot')\n",
    "plt.show()\n",
    "# show recall\n",
    "data = np.concatenate((recall0.reshape((num, 1)), recall1.reshape((num, 1)), recall2.reshape((num, 1)), recall3.reshape((num, 1))), axis=1)\n",
    "data = pd.DataFrame(data, columns=['Background','CSF','GM','WM'])\n",
    "data.boxplot(sym = 'o', vert = True, whis=1.5, patch_artist = True, meanline = False, showmeans = True, showbox = True, showfliers = True)\n",
    "plt.title('Recall Plot')\n",
    "plt.show()\n",
    "# show Dice/F1\n",
    "data = np.concatenate((f10.reshape((num, 1)), f11.reshape((num, 1)), f12.reshape((num, 1)), f13.reshape((num, 1))), axis=1)\n",
    "data = pd.DataFrame(data, columns=['Background','CSF','GM','WM'])\n",
    "data.boxplot(sym = 'o', vert = True, whis=1.5, patch_artist = True, meanline = False, showmeans = True, showbox = True, showfliers = True)\n",
    "plt.title('Dice/F1 Plot')\n",
    "plt.show()\n",
    "\n",
    "        \n",
    "loss_test /= sum_pts\n",
    "\n",
    "print('+ TESTING \\tLoss: {:.6f}'.format(loss_test))\n",
    "\n",
    "# Show last testing sample as an example\n",
    "print('\\n\\nReference segmentation')\n",
    "display_image(sitk.LabelToRGB(sample['seg']))\n",
    "print('Predicted segmentation')\n",
    "display_image(sitk.LabelToRGB(prediction))\n",
    "\n",
    "print('\\nFinished TESTING.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating relative tissue volume\n",
    "\n",
    "import os\n",
    "\n",
    "# USE THIS TO RUN THE CALCULATIONS ON SEGMENTATONS\n",
    "seg_dir = './output/final_test/'\n",
    "\n",
    "# USE THIS TO RUN THE CALCULATIONS ON REFERENCE SEGMENTATIONS\n",
    "# seg_dir = data_dir + 'segs_refs/'\n",
    "\n",
    "meta_data_reg_train = pd.read_csv(data_dir + 'meta/meta_data_regression_test.csv')\n",
    "\n",
    "ids_reg_train = list(meta_data_reg_train['subject_id'])\n",
    "files_reg_seg_train = [seg_dir + 'sub-' + f + '_T1w_seg.nii.gz' for f in ids_reg_train]\n",
    "\n",
    "# THIS MATRIX WILL STORE THE VOLUMES PER TISSUE CLASS\n",
    "vols = np.zeros((3,len(files_reg_seg_train)))\n",
    "\n",
    "for idx, _ in enumerate(tqdm(range(len(files_reg_seg_train)), desc='Calculating Features')):\n",
    "    \n",
    "    seg_filename = files_reg_seg_train[idx]\n",
    "    \n",
    "    if os.path.exists(seg_filename):\n",
    "        seg = sitk.ReadImage(seg_filename)\n",
    "        \n",
    "        seg = sitk.GetArrayFromImage(seg)\n",
    "        total = seg.shape[0]*seg.shape[1]*seg.shape[2]\n",
    "\n",
    "        # relative tissue volumes for GM, WM and CSF\n",
    "        num_csf = np.sum(seg == 1) / total\n",
    "        num_gm = np.sum(seg == 2) / total\n",
    "        num_wm = np.sum(seg == 3) / total\n",
    "\n",
    "        vols[0][idx] = num_csf\n",
    "        vols[1][idx] = num_gm\n",
    "        vols[2][idx] = num_wm\n",
    "\n",
    "plt.scatter(vols[0,:],meta_data_reg_train['age'], marker='.')\n",
    "plt.scatter(vols[1,:],meta_data_reg_train['age'], marker='.')\n",
    "plt.scatter(vols[2,:],meta_data_reg_train['age'], marker='.')\n",
    "plt.grid()\n",
    "plt.title('Unnormalised')\n",
    "plt.xlabel('Volume')\n",
    "plt.ylabel('Age')\n",
    "plt.legend(('CSF','GM','WM'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating absolute tissue volume\n",
    "\n",
    "vols_normalised = np.zeros((3,len(files_reg_seg_train)))\n",
    "vols_normalised = np.log(vols)\n",
    "\n",
    "mean = np.mean(vols_normalised)\n",
    "std = np.std(vols_normalised)\n",
    "vols_normalised = (vols_normalised - mean) / std\n",
    "\n",
    "# to 0-1\n",
    "min = np.min(vols_normalised)\n",
    "max = np.max(vols_normalised)\n",
    "vols_normalised = (vols_normalised - min) / (max - min)\n",
    "\n",
    "plt.scatter(vols_normalised[0,:],meta_data_reg_train['age'], marker='.')\n",
    "plt.scatter(vols_normalised[1,:],meta_data_reg_train['age'], marker='.')\n",
    "plt.scatter(vols_normalised[2,:],meta_data_reg_train['age'], marker='.')\n",
    "plt.grid()\n",
    "plt.title('Normalised')\n",
    "plt.xlabel('Volume')\n",
    "plt.ylabel('Age')\n",
    "plt.legend(('CSF','GM','WM'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vols_normalised.T\n",
    "y = meta_data_reg_train['age'].values.reshape(-1,1)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "min = 10\n",
    "max = 100\n",
    "predicted = best_nn.predict(X)\n",
    "predicted = predicted * (max - min) + min\n",
    "\n",
    "print('MAE: {0}'.format(mean_absolute_error(y,predicted)))\n",
    "print('R2: {0}'.format(r2_score(y,predicted)))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y, predicted, marker='.')\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
    "ax.set_xlabel('Real Age')\n",
    "ax.set_ylabel('Predicted Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use CNN directly to predict age from 3D scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# data directory\n",
    "data_dir = 'data/brain_age/'\n",
    "\n",
    "# Read the meta data using pandas\n",
    "import pandas as pd\n",
    "meta_data_all = pd.read_csv(data_dir + 'meta/meta_data_all.csv')\n",
    "meta_data_train = pd.read_csv(data_dir + 'meta/meta_data_regression_train.csv')\n",
    "meta_data_all.head() # show the first five data entries\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "meta_data = meta_data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "from IPython.display import display\n",
    "\n",
    "# Calculate parameters low and high from window and level\n",
    "def wl_to_lh(window, level):\n",
    "    low = level - window/2\n",
    "    high = level + window/2\n",
    "    return low,high\n",
    "\n",
    "def display_image(img, x=None, y=None, z=None, window=None, level=None, colormap='gray', crosshair=False):\n",
    "    # Convert SimpleITK image to NumPy array\n",
    "    img_array = sitk.GetArrayFromImage(img)\n",
    "    \n",
    "    # Get image dimensions in millimetres\n",
    "    size = img.GetSize()\n",
    "    spacing = img.GetSpacing()\n",
    "    width  = size[0] * spacing[0]\n",
    "    height = size[1] * spacing[1]\n",
    "    depth  = size[2] * spacing[2]\n",
    "    \n",
    "    if x is None:\n",
    "        x = np.floor(size[0]/2).astype(int)\n",
    "    if y is None:\n",
    "        y = np.floor(size[1]/2).astype(int)\n",
    "    if z is None:\n",
    "        z = np.floor(size[2]/2).astype(int)\n",
    "    \n",
    "    if window is None:\n",
    "        window = np.max(img_array) - np.min(img_array)\n",
    "    \n",
    "    if level is None:\n",
    "        level = window / 2 + np.min(img_array)\n",
    "    \n",
    "    low,high = wl_to_lh(window,level)\n",
    "\n",
    "    # Display the orthogonal slices\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 4))\n",
    "\n",
    "    ax1.imshow(img_array[z,:,:], cmap=colormap, clim=(low, high), extent=(0, width, height, 0))\n",
    "    ax2.imshow(img_array[:,y,:], origin='lower', cmap=colormap, clim=(low, high), extent=(0, width,  0, depth))\n",
    "    ax3.imshow(img_array[:,:,x], origin='lower', cmap=colormap, clim=(low, high), extent=(0, height, 0, depth))\n",
    "\n",
    "    # Additionally display crosshairs\n",
    "    if crosshair:\n",
    "        ax1.axhline(y * spacing[1], lw=1)\n",
    "        ax1.axvline(x * spacing[0], lw=1)\n",
    "        ax2.axhline(z * spacing[2], lw=1)\n",
    "        ax2.axvline(x * spacing[0], lw=1)\n",
    "        ax3.axhline(z * spacing[2], lw=1)\n",
    "        ax3.axvline(y * spacing[1], lw=1)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def interactive_view(img):\n",
    "    size = img.GetSize() \n",
    "    img_array = sitk.GetArrayFromImage(img)\n",
    "    interact(display_image,img=fixed(img),\n",
    "             x=(0, size[0] - 1),\n",
    "             y=(0, size[1] - 1),\n",
    "             z=(0, size[2] - 1),\n",
    "             window=(0,np.max(img_array) - np.min(img_array)),\n",
    "             level=(np.min(img_array),np.max(img_array)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_mean_unit_var(image, mask):\n",
    "    \"\"\"Normalizes an image to zero mean and unit variance.\"\"\"\n",
    "\n",
    "    img_array = sitk.GetArrayFromImage(image)\n",
    "    img_array = img_array.astype(np.float32)\n",
    "\n",
    "    msk_array = sitk.GetArrayFromImage(mask)\n",
    "\n",
    "    mean = np.mean(img_array[msk_array>0])\n",
    "    std = np.std(img_array[msk_array>0])\n",
    "\n",
    "    if std > 0:\n",
    "        img_array = (img_array - mean) / std\n",
    "        img_array[msk_array==0] = 0\n",
    "\n",
    "    image_normalised = sitk.GetImageFromArray(img_array)\n",
    "    image_normalised.CopyInformation(image)\n",
    "\n",
    "    return image_normalised\n",
    "\n",
    "\n",
    "def resample_image(image, out_spacing=(1.0, 1.0, 1.0), out_size=None, is_label=False, pad_value=0):\n",
    "    \"\"\"Resamples an image to given element spacing and output size.\"\"\"\n",
    "\n",
    "    original_spacing = np.array(image.GetSpacing())\n",
    "    original_size = np.array(image.GetSize())\n",
    "\n",
    "    if out_size is None:\n",
    "        out_size = np.round(np.array(original_size * original_spacing / np.array(out_spacing))).astype(int)\n",
    "    else:\n",
    "        out_size = np.array(out_size)\n",
    "\n",
    "    original_direction = np.array(image.GetDirection()).reshape(len(original_spacing),-1)\n",
    "    original_center = (np.array(original_size, dtype=float) - 1.0) / 2.0 * original_spacing\n",
    "    out_center = (np.array(out_size, dtype=float) - 1.0) / 2.0 * np.array(out_spacing)\n",
    "\n",
    "    original_center = np.matmul(original_direction, original_center)\n",
    "    out_center = np.matmul(original_direction, out_center)\n",
    "    out_origin = np.array(image.GetOrigin()) + (original_center - out_center)\n",
    "\n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetOutputSpacing(out_spacing)\n",
    "    resample.SetSize(out_size.tolist())\n",
    "    resample.SetOutputDirection(image.GetDirection())\n",
    "    resample.SetOutputOrigin(out_origin.tolist())\n",
    "    resample.SetTransform(sitk.Transform())\n",
    "    resample.SetDefaultPixelValue(pad_value)\n",
    "\n",
    "    if is_label:\n",
    "        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n",
    "    else:\n",
    "        resample.SetInterpolator(sitk.sitkBSpline)\n",
    "\n",
    "    return resample.Execute(image)\n",
    "\n",
    "\n",
    "class ImageRegressionDataset(Dataset):\n",
    "    \"\"\"Dataset for image regression.\"\"\"\n",
    "\n",
    "    def __init__(self, file_list_img,file_list_msk, label_list_age,img_spacing, img_size):\n",
    "        self.samples = []\n",
    "        self.img_names = []\n",
    "        self.labels = []\n",
    "        for idx, _ in enumerate(tqdm(range(len(file_list_img)), desc='Loading Data')):\n",
    "            img_path = file_list_img[idx]\n",
    "#             seg_path = file_list_seg[idx]\n",
    "            msk_path = file_list_msk[idx]\n",
    "\n",
    "            img = sitk.ReadImage(img_path, sitk.sitkFloat32)\n",
    "\n",
    "#             seg = sitk.ReadImage(seg_path, sitk.sitkInt64)\n",
    "\n",
    "            msk = sitk.ReadImage(msk_path, sitk.sitkUInt8)\n",
    "\n",
    "            #pre=processing\n",
    "            img = zero_mean_unit_var(img, msk)\n",
    "            img = resample_image(img, img_spacing, img_size, is_label=False)\n",
    "\n",
    "            \n",
    "\n",
    "            self.samples.append(img)\n",
    "            self.img_names.append(os.path.basename(img_path))\n",
    "            self.labels.append(label_list_age[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        sample = self.samples[item]\n",
    "\n",
    "        image = torch.from_numpy(sitk.GetArrayFromImage(sample)).unsqueeze(0)\n",
    "\n",
    "        return image,self.labels[item]\n",
    "\n",
    "    def get_sample(self, item):\n",
    "        return self.samples[item]\n",
    "\n",
    "    def get_img_name(self, item):\n",
    "        return self.img_names[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "cuda_dev = '0' #GPU device 0 (can be changed if multiple GPUs are available)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:\" + cuda_dev if use_cuda else \"cpu\")\n",
    "\n",
    "print('Device: ' + str(device))\n",
    "if use_cuda:\n",
    "    print('GPU: ' + str(torch.cuda.get_device_name(int(cuda_dev))))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_seed = 42 #fixed random seed\n",
    "\n",
    "img_size = [96, 96, 96]\n",
    "img_spacing = [2, 2, 2]\n",
    "\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "batch_size = 25\n",
    "val_interval = 10\n",
    "\n",
    "out_dir = './output'\n",
    "\n",
    "# Create output directory\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_regression_train = pd.read_csv(data_dir + 'meta/meta_data_regression_train.csv')\n",
    "ids_regression_train = list(meta_data_regression_train['subject_id'])\n",
    "ages_regression_train = list(meta_data_regression_train['age'])\n",
    "files_regression_img_train = [data_dir + 'images/sub-' + f + '_T1w_unbiased.nii.gz' for f in ids_regression_train]\n",
    "files_regression_msk_train = [data_dir + 'masks/sub-' + f + '_T1w_brain_mask.nii.gz' for f in ids_regression_train]\n",
    "\n",
    "meta_data_regression_test = pd.read_csv(data_dir + 'meta/meta_data_regression_test.csv')\n",
    "ids_regression_test = list(meta_data_regression_test['subject_id'])\n",
    "ages_regression_test = list(meta_data_regression_test['age'])\n",
    "files_regression_img_test = [data_dir + 'images/sub-' + f + '_T1w_unbiased.nii.gz' for f in ids_regression_test]\n",
    "files_regression_msk_test = [data_dir + 'masks/sub-' + f + '_T1w_brain_mask.nii.gz' for f in ids_regression_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "dataset_train = ImageRegressionDataset(files_regression_img_train, files_regression_msk_train,ages_regression_train, img_spacing, img_size)\n",
    "\n",
    "train_proportion = 0.5\n",
    "train_examples_fold_0 = round(len(dataset_train)*train_proportion)\n",
    "train_examples_fold_1 = len(dataset_train) - train_examples_fold_0\n",
    "\n",
    "dataset_train_fold_0, dataset_train_fold_1 = random_split(dataset_train,\n",
    "                                           (train_examples_fold_0,\n",
    "                                            train_examples_fold_1))\n",
    "\n",
    "dataloader_train_fold_0 = torch.utils.data.DataLoader(dataset_train_fold_0, batch_size=batch_size, shuffle=True)\n",
    "dataloader_train_fold_1 = torch.utils.data.DataLoader(dataset_train_fold_1, batch_size=batch_size, shuffle=True)\n",
    "dataloaders_train = [dataloader_train_fold_0,dataloader_train_fold_1]\n",
    "dataloader_train_all = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testing data\n",
    "dataset_test = ImageRegressionDataset(files_regression_img_test, files_regression_msk_test,ages_regression_test, img_spacing, img_size)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 3D CNN\n",
    "\n",
    "class CNN3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN3D, self).__init__()\n",
    "\n",
    "        self.DCNN = nn.Sequential(\n",
    "            # input shape (batch_size, 1, 96, 96, 96)\n",
    "            nn.Conv3d(1, 6, kernel_size=5, bias=True, padding=0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.MaxPool3d(2, stride = 2),\n",
    "\n",
    "\n",
    "            nn.Conv3d(6, 16, kernel_size=5, bias=True, padding=0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.MaxPool3d(2, stride = 2),\n",
    "\n",
    "\n",
    "            nn.Conv3d(16, 120, kernel_size=5, bias=True, padding=0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "         \n",
    "\n",
    "            nn.Conv3d(120, 84, kernel_size=3, stride = 2, bias=True, padding=0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm3d(84),\n",
    "\n",
    "            nn.Conv3d(84, 32, kernel_size=3, bias=True, padding=0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm3d(32),\n",
    "\n",
    "            nn.Conv3d(32, 8, kernel_size=3, bias=True, padding=0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm3d(8),\n",
    "\n",
    "            nn.Conv3d(8, 1, kernel_size=4, bias=True, padding=0),\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x, label=None):\n",
    "\n",
    "        out = self.DCNN(x)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(out, label):\n",
    "#     loss_func = nn.MSELoss()\n",
    "    loss_func = nn.L1Loss()\n",
    "    loss = loss_func(out, label)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two fold cross validation\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 7))\n",
    "\n",
    "num_epochs = 50\n",
    "learning_rate = 0.0008\n",
    "val_interval = 5\n",
    "milestones = [8, 18, 35]\n",
    "gamma = 0.5\n",
    "\n",
    "\n",
    "for fold in range(2):\n",
    "    dataloader_train = dataloaders_train[fold]\n",
    "    dataloader_val = dataloaders_train[1-fold]\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    model = CNN3D().to(device)\n",
    "    print(model)\n",
    "    \n",
    "    beta1 = 0.9\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
    "    scheduler = MultiStepLR(optimizer, milestones, gamma = gamma)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss_record = 0\n",
    "        val_loss_record = 0\n",
    "        train_count = 0\n",
    "        val_count = 0\n",
    "\n",
    "        for data,label in dataloader_train:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.to(device)).reshape(-1,)\n",
    "            train_loss = loss_function(output,label.to(torch.float).to(device))\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_record += train_loss.item()\n",
    "            train_count += 1\n",
    "        \n",
    "        train_loss_record /= train_count\n",
    "        train_losses.append(train_loss_record)\n",
    "        \n",
    "        \n",
    "        label_all = []\n",
    "        output_all = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data,label in dataloader_val:\n",
    "                output = model(data.to(device)).reshape(-1,)\n",
    "                val_loss = loss_function(output,label.to(torch.float).to(device))\n",
    "                val_loss_record += val_loss.item()\n",
    "                val_count += 1\n",
    "                if epoch == num_epochs-1:\n",
    "                    label_all += list(label.numpy())\n",
    "                    output_all += list(output.to('cpu').numpy())\n",
    "            val_loss_record /= val_count\n",
    "            val_losses.append(val_loss_record)\n",
    "        print(f\"EPOCH: {epoch+1} --- the train MAE is {train_loss_record:.4f}, the val MAE is {val_loss_record:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        if epoch == num_epochs-1:\n",
    "            label_all = np.array(label_all)\n",
    "            output_all = np.array(output_all)\n",
    "            axs[1][fold].scatter(label_all, output_all, marker='.')\n",
    "            axs[1][fold].plot([label_all.min(), label_all.max()], [label_all.min(), label_all.max()], 'k--', lw=2)\n",
    "            axs[1][fold].set_xlabel('Real Age')\n",
    "            axs[1][fold].set_ylabel('Predicted Age')\n",
    "    \n",
    "    axs[0][fold].plot(train_losses)\n",
    "    axs[0][fold].plot(val_losses)\n",
    "    axs[0][fold].legend(['train','val'])\n",
    "    \n",
    "    \n",
    "fig.savefig('two_fold.png')\n",
    "fig.show()\n",
    "#     plt.figure()\n",
    "#     plt.plot(train_losses)\n",
    "#     plt.plot(val_losses)\n",
    "#     plt.legend(['train','val'])\n",
    "#     plt.savefig('fold'+str(fold)+'.png')\n",
    "#     plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "model = CNN3D().to(device)\n",
    "print(model)\n",
    "\n",
    "beta1 = 0.5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss_sum = 0\n",
    "    for data,label in dataloader_train_all:\n",
    "        model.zero_grad()\n",
    "        output = model(data.to(device)).reshape(-1,)\n",
    "        train_loss = loss_function(output,label.to(torch.float).to(device))\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_sum += train_loss.item()\n",
    "    train_loss = train_loss_sum/len(dataloader_train_all)\n",
    "    train_losses.append(train_loss)\n",
    "    with torch.no_grad():\n",
    "        test_loss_sum = 0\n",
    "        for data,label in dataloader_test:\n",
    "            output = model(data.to(device)).reshape(-1)\n",
    "            test_loss = loss_function(output,label.to(torch.float).to(device))\n",
    "            test_loss_sum += test_loss.item()\n",
    "        test_loss = test_loss_sum/len(dataloader_test)\n",
    "        test_losses.append(test_loss)\n",
    "    print(f\"EPOCH: {epoch+1} --- the train MAE is {train_losses[-1]:.4f}, the val MAE is {test_losses[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_losses)\n",
    "plt.plot(test_losses)\n",
    "plt.legend(['train','test'])\n",
    "plt.savefig('test.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_age = []\n",
    "output_all = []\n",
    "with torch.no_grad():\n",
    "    for data,label in dataloader_test:\n",
    "        real_age += list(label.numpy())\n",
    "        output = model(data.to(device)).reshape(-1).to('cpu')\n",
    "        output_all += list(output.numpy())\n",
    "real_age = np.array(real_age)\n",
    "output_all = np.array(output_all)\n",
    "\n",
    "# print('MAE: {0}'.format(mean_absolute_error(real_age,output)))\n",
    "# print('R2: {0}'.format(r2_score(real_age,output)))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(real_age, output_all, marker='.')\n",
    "ax.plot([real_age.min(), real_age.max()], [real_age.min(), real_age.max()], 'k--', lw=2)\n",
    "ax.set_xlabel('Real Age')\n",
    "ax.set_ylabel('Predicted Age')\n",
    "plt.savefig('scatter_part_b.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
